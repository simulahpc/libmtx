@c This file is part of libmtx.
@c Copyright (C) 2022 James D. Trotter
@c
@c libmtx is free software: you can redistribute it and/or modify it
@c under the terms of the GNU General Public License as published by
@c the Free Software Foundation, either version 3 of the License, or
@c (at your option) any later version.
@c
@c libmtx is distributed in the hope that it will be useful, but
@c WITHOUT ANY WARRANTY; without even the implied warranty of
@c MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
@c General Public License for more details.
@c
@c You should have received a copy of the GNU General Public License
@c along with libmtx.  If not, see <https://www.gnu.org/licenses/>.
@c
@c Authors: James D. Trotter <james@simula.no>
@c Last modified: 2022-01-19
@c
@c libmtx User Guide: Distributed matrices and vectors.

@node Distributed matrices and vectors
@chapter Distributed matrices and vectors
This chapter describes how to distribute matrices and vectors across
multiple processes, and how to perform various linear algebra
operations on distributed matrices and vectors. The distributed-memory
computing features in libmtx are implemented using MPI, and,
therefore, you will need to build libmtx with MPI support. In many
cases, working with distributed matrices and vectors involves
converting to or from Matrix Market format, and it may therefore be a
good idea to read @ref{Distributed Matrix Market files} to understand
how to work with distributed Matrix Market files in libmtx.

@menu
* Distributed vectors:: Data structures for distributed vectors.
* Distributed matrices:: Data structures for distributed matrices.
@end menu


@node Distributed vectors
@section Distributed vectors
@cindex distributed vector
@tindex struct mtxdistvector
The file @file{libmtx/vector/distvector.h} defines the type
@code{struct mtxdistvector}. This data type builds on top of
@code{struct mtxvector} (see @ref{Vectors}) to provide different
options for the underlying storage and implementation of vector
operations.

The definition of @code{struct mtxdistvector} is shown below.
@example
@code{struct mtxdistvector @{
    MPI_Comm comm;
    int comm_size;
    int rank;
    struct mtxpartition rowpart;
    struct mtxvector interior;
@};}
@end example
Similar to @code{mtxdistfile} (see @ref{Data structures for
distributed Matrix Market files}), the first three struct members of
@code{mtxdistvector} contain information about the group of processes
sharing the distributed vector. This includes their MPI communicator
(@code{comm}), the number of processes (@code{comm_size}) and the rank
of the current process (@code{rank}). Also, @code{rowpart} defines a
partitioning of the vector elements. This is used to determine which
parts of the vector belongs to which process.

The final struct member, @code{interior}, stores the part of the
distributed vector that resides on the current process.


@node Creating distributed vectors
@subsection Creating distributed vectors
@cindex free
@findex mtxdistvector_free
This section describes how to create distributed vectors. But first,
the function @code{mtxdistvector_free} is used to free storage allocated
for a distributed vector.
@example
@code{void mtxdistvector_free(struct mtxdistvector * distvector);}
@end example

@cindex copy
@findex mtxdistvector_init_copy
To create a copy of an existing distributed vector, use the function
@code{mtxdistvector_init_copy}.
@example
@code{int mtxdistvector_init_copy(
    struct mtxdistvector * dst,
    const struct mtxdistvector * src);}
@end example

@cindex copy
@findex mtxdistvector_alloc_copy
If storage for a copy of an existing distributed vector is needed, but
the vector values should not be copied or initialised, use the
function @code{mtxdistvector_alloc_copy}.
@example
@code{int mtxdistvector_alloc_copy(
    struct mtxdistvector * dst,
    const struct mtxdistvector * src);}
@end example

@cindex allocate
@cindex array format
@cindex coordinate format
@findex mtxdistvector_alloc_array
@findex mtxdistvector_alloc_coordinate
To allocate storage for a distributed vector in @emph{array} or
@emph{coordinate} format, the functions
@code{mtxdistvector_alloc_array} or
@code{mtxdistvector_alloc_coordinate} may be used.
@example
@code{int mtxdistvector_alloc_array(
    struct mtxdistvector * vector,
    enum mtxfield field,
    enum mtxprecision precision,
    int num_rows,
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);

int mtxdistvector_alloc_coordinate(
    struct mtxdistvector * vector,
    enum mtxfield field,
    enum mtxprecision precision,
    int num_rows,
    int64_t num_nonzeros,
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
@noindent
In both cases, the desired field and precision must be specified, as
well as the number of rows in the entire, global vector
(@code{num_rows}). For a vector in coordinate format, it is also
necessary to specify the number of nonzero vector elements
(@code{num_nonzeros}) that will reside @emph{on the current process}.
Note that the vector values are not initialised, and so it is up to
the user to initialise them.

@code{rowpart} must be a partitioning of the rows of the global
vector, which means that @code{rowpart->size} must be equal to
@samp{num_rows}. There may be at most one part in the partition per
MPI process in the communicator @code{comm}.

@findex mtxdistvector_init_@var{type}_@var{field}_@var{precision}
If the vector values are already known, then there are also functions
for allocating a distributed vector and initialising the values
directly. This is done by calling
@code{mtxdistvector_init_@var{type}_@var{field}_@var{precision}},
where @code{@var{type}}, @code{@var{field}} and @code{@var{precision}}
denote the vector type (i.e., @samp{array} or @samp{coordinate}),
field (i.e., @samp{real}, @samp{complex} or @samp{integer}) and
precision (i.e., @samp{single} or @samp{double}).

@findex mtxdistvector_init_array_complex_double
For example, to create a double precision, complex vector in array
format, use @code{mtxdistvector_init_array_complex_double}.
@example
@code{int mtxdistvector_init_array_complex_double(
    struct mtxdistvector * distvector,
    int num_rows,
    const double (* data)[2],
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
@noindent
Each process provides its local vector entries in the array
@code{data}. The length of the @code{data} array must be at least
@samp{rowpart->part_sizes[rank]}, where @samp{rank} is the rank of the
current process. (If there are fewer parts in the partition than MPI
processes, then the @code{data} array is not used on processes where
@samp{rank} is greater than or equal to
@samp{rowpart->num_parts}. @code{data} may therefore be set to
@samp{NULL} on those processes.)

@findex mtxdistvector_init_coordinate_complex_double
To create a double precision, complex vector in coordinate format, use
@code{mtxdistvector_init_coordinate_complex_double}.
@example
@code{int mtxdistvector_init_coordinate_complex_double(
    struct mtxdistvector * vector,
    int num_rows,
    int64_t num_nonzeros,
    const int * idx,
    const double (* data)[2],
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
@noindent
The arguments @code{idx} and @code{data} are arrays of length
@code{num_nonzeros}. Each process may provide arrays of different
lengths. Each index @samp{idx[0]}, @samp{idx[1]}, ...,
@samp{idx[num_nonzeros-1]}, is an integer in the range
@samp{[0,N)}, where @samp{N} is the size of the part owned by the
current process, (i.e., @samp{rowpart->part_sizes[rank]}, where
@samp{rank} is the rank of the current process).

Note that duplicate entries are allowed, but this may cause some
operations (e.g., @code{mtxdistvector_dot}, @code{mtxdistvector_nrm2})) to
produce incorrect results.


@node Modifying values of distributed vectors
@subsection Modifying values
@findex mtxdistvector_set_constant_@var{field}_@var{precision}
The functions
@code{mtxdistvector_set_constant_@var{field}_@var{precision}} can be
used to set every (nonzero) value of a vector equal to a constant
scalar, where @code{@var{field}} and @code{@var{precision}} should
match the field (i.e., @samp{real}, @samp{complex} or @samp{integer})
and precision (i.e., @samp{single} or @samp{double}) of
@code{mtxdistvector}.
@findex mtxdistvector_set_constant_@var{field}_@var{precision}
@findex mtxdistvector_set_constant_real_single
@findex mtxdistvector_set_constant_real_double
@findex mtxdistvector_set_constant_complex_single
@findex mtxdistvector_set_constant_complex_double
@findex mtxdistvector_set_constant_integer_single
@findex mtxdistvector_set_constant_integer_double
@example
@code{int mtxdistvector_set_constant_real_single(
    struct mtxdistvector *, float a, struct mtxdisterror * disterr);
int mtxdistvector_set_constant_real_double(
    struct mtxdistvector *, double a, struct mtxdisterror * disterr);
int mtxdistvector_set_constant_complex_single(
    struct mtxdistvector *, float a[2], struct mtxdisterror * disterr);
int mtxdistvector_set_constant_complex_double(
    struct mtxdistvector *, double a[2], struct mtxdisterror * disterr);
int mtxdistvector_set_constant_integer_single(
    struct mtxdistvector *, int32_t a, struct mtxdisterror * disterr);
int mtxdistvector_set_constant_integer_double(
    struct mtxdistvector *, int64_t a, struct mtxdisterror * disterr);}
@end example

To access or modify individual vector elements, the underlying vector
storage is accessed through the appropriate member of the
@code{storage} union in the @code{mtxvector} struct.


@node Converting distributed vectors to and from Matrix Market format
@subsection Converting to and from Matrix Market format
@cindex convert to and from Matrix Market format
@cindex convert to and from distributed Matrix Market format
A distributed vector can be obtained from a Matrix Market file by
distributing the Matrix Market entries across multiple processes
before converting the data on each process to the desired vector
storage format. Typically, this involves partitioning the rows of the
vector and distributing the data accordingly. If the Matrix Market
file is already distributed across several processes, then the data is
partitioned and redistributed before converting to the desired vector
storage format.

Conversely, a distributed vector can be converted directly to
distributed Matrix Market format without the need for redistributing
any data. If desirable, the data may also be gathered onto a single,
root process after converting to Matrix Market format. In either case,
converting to Matrix Market format allows the data to be easily
written to a Matrix Market file.

@findex mtxdistvector_from_mtxfile
To convert a vector in Matrix Market format to @code{struct
mtxdistvector}, the function @code{mtxdistvector_from_mtxfile} can be
used. In this case, the Matrix Market file @code{mtxfile} must reside
on the process whose rank is @code{root}.
@example
@code{int mtxdistvector_from_mtxfile(
    struct mtxdistvector * dst,
    const struct mtxfile * src,
    enum mtxvectortype type,
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    int root,
    struct mtxdisterror * disterr);}
@end example
@noindent
The @code{type} argument may be used to specify a desired storage
format or implementation for the underlying @code{mtxvector} on each
process. If @code{type} is @samp{mtxvector_auto}, then the type of
@code{mtxvector} is chosen to match the type of @code{src}. That is,
@samp{mtxvector_array} is used if @code{src} is in array format, and
@samp{mtxvector_coordinate} is used if @code{src} is in coordinate
format.

Furthermore, @code{rowpart} must be a partitioning of the rows of the
global vector. Therefore, @code{rowpart->size} must be equal to the
number of rows in the underlying vector represented by
@code{mtxfile}. The partition must consist of at most one part for
each MPI process in the communicator @code{comm}. If @code{rowpart} is
@samp{NULL}, then the rows are partitioned into contiguous blocks of
equal size by default.


@findex mtxdistvector_to_mtxfile
To convert @code{struct mtxdistvector} back to Matrix Market format,
the function @code{mtxdistvector_to_mtxfile} can be used.
@example
@code{int mtxdistvector_to_mtxfile(
    const struct mtxdistvector * mtxdistvector,
    struct mtxfile * mtxfile,
    enum mtxfileformat mtxfmt,
    MPI_Comm comm,
    int root,
    struct mtxdisterror * disterr);}
@end example
@noindent
The resulting Matrix Market file resides on the process whose rank is
@code{root}. The vector is stored in array format if @code{mtxfmt} is
@samp{mtxfile_array} or in coordinate format if @code{mtxfmt} is
@samp{mtxfile_coordinate}.


@findex mtxdistvector_from_mtxdistfile
If a Matrix Market file has already been distributed among multiple
processes, then @code{mtxdistvector_from_mtxdistfile} can be used to
obtain a distributed vector with the desired partitioning and storage
format.
@example
@code{int mtxdistvector_from_mtxdistfile(
    struct mtxdistvector * distvector,
    const struct mtxdistfile * mtxdistfile,
    enum mtxvectortype vector_type,
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
@noindent
Each process partitions its part of the distributed Matrix Market
file. The data is then redistributed after partitioning.


@findex mtxdistvector_to_mtxdistfile
The function @code{mtxdistvector_to_mtxdistfile} will convert a
distributed vector to a distributed Matrix Market format.
@example
@code{int mtxdistvector_to_mtxdistfile(
    const struct mtxdistvector * distvector,
    struct mtxdistfile * mtxdistfile,
    struct mtxdisterror * disterr);}
@end example
@noindent
In this case, there is no redistribution or communication of the
underlying data.


@node Halo exchange
@subsection Halo exchange
@cindex halo exchange
Some linear algebra operations, such as matrix-vector multiplication,
require communication between MPI processes whenever distributed
matrices and vectors are used. This kind of communication is commonly
implemented through a @dfn{halo exchange}. This section describes data
structures for representing vector halos, and how to perform halo
exchanges.


@node Distributed matrices
@section Distributed matrices
@cindex distributed matrix
@tindex struct mtxdistmatrix
The file @file{libmtx/distmatrix/distmatrix.h} defines the type
@code{struct mtxdistmatrix}. This data type builds on top of
@code{struct mtxmatrix} (see @ref{Matrices}) to offer different options
for the underlying storage and implementation of matrix operations.
@example
@code{struct mtxdistmatrix @{
    MPI_Comm comm;
    int comm_size;
    int rank;
    struct mtxmatrix interior;
@};}
@end example
@noindent
Similar to @code{mtxdistfile} (see @ref{Data structures for
distributed Matrix Market files}), The first three struct members of
@code{mtxdistmatrix} contain information about the group of processes
sharing the distributed matrix. This includes their MPI communicator
(@code{comm}), the number of processes (@code{comm_size}) and the rank
of the current process (@code{rank}). The matrix, @code{interior},
stores the part of the distributed matrix that resides on the current
process.



@c @menu
@c * Data types:: Basic data types for representing distributed matrices and vectors.
@c * Creating distributed matrices and vectors:: Functions for creating distributed matrices and vectors.
@c @end menu


@c @node Data types
@c @section Data types

@c This section describes the basic data types used to represent
@c distributed matrices and vectors.

@c @tindex struct mtxdist
@c @tindex mtxdist
@c The file @file{libmtx/mtxdist.h} defines the @code{struct
@c mtxdist} type, which is used to represent distributed objects in the
@c Matrix Market format. The definition of the @code{mtxdist} struct is shown
@c below.
@c @example
@c @code{struct mtxdist @{
@c   /* Data distribution */
@c   MPI_Comm comm;
@c   enum mtx_distribution row_distribution;
@c   enum mtx_distribution column_distribution;
@c   int64_t num_global_rows;
@c   int64_t num_global_columns;
@c   int num_block_rows;
@c   int num_block_columns;
@c   int block_row_size;
@c   int block_column_size;
@c   int block_row;
@c   int block_column;
@c   int64_t * global_rows;
@c   int64_t * global_columns;

@c   /* Matrix Market object */
@c   struct mtx * mtx;
@c @};}
@c @end example

@c The @code{mtxdist} struct contains information about how the
@c underlying matrix or vector is distributed among processes.  It also
@c contains a member of type @code{struct mtx}, which, on a given MPI
@c process, represents the underlying, local matrix of the current
@c process.


@c The following sections provide a detailed explanation of the
@c @code{mtxdist} struct members and their data types.


@c @node Data distribution
@c @subsection Data distribution

@c @cindex data distribution
@c @cindex distributed matrix
@c @cindex distributed vector
@c @cindex block distribution
@c @cindex cyclic distribution
@c @cindex block-cyclic distribution
@c @cindex discrete distribution
@c @tindex mtx_distribution
@c It is often necessary to distribute large matrices and vectors across
@c multiple processes, both for the purpose of performing computations in
@c parallel and also to use multiple nodes, thereby increasing the total
@c amount of available memory.  To facilitate such data distribution,
@c some additional information is stored in the @code{mtx} struct.

@c First, we define the additional enum type @code{mtx_distribution},
@c which describes different methods for distributing a one-dimensional
@c data structure, such as a vector, among multiple processes.  Matrices
@c are distributed by independently specifying the distributions of the
@c rows and columns.
@c @example
@c @code{enum mtx_distribution @{
@c     mtx_private,           /* owned by a single process */
@c     mtx_replicated,        /* replicated across every process */
@c     mtx_block,             /* block distribution */
@c     mtx_cyclic,            /* cyclic distribution */
@c     mtx_block_cyclic,      /* block-cyclic distribution */
@c     mtx_discrete,          /* discrete distribution */
@c @};}
@c @end example
@c By default, matrices and vectors are not distributed
@c (@code{mtx_private}).  That is, the entries of a vector and the rows
@c and columns of a matrix are owned by a single process.

@c For a distributed vector, @code{mtx_block} is used when the vector is
@c partitioned into contiguous blocks of roughly equal size and one block
@c is assigned to each process.  In contrast, @code{mtx_cyclic} assigns
@c consecutive entries of the vector to successive processes.  By
@c generalising the block and cyclic distributions,
@c @code{mtx_block_cyclic} assigns consecutive, fixed-size blocks to
@c successive processes.  Finally, @code{mtx_discrete} allows an
@c arbitrary assignment of global vector entries to processes.


@c @cindex cover
@c @cindex partition
@c @tindex mtxpartitioning
@c The enum type @code{mtxpartitioning}, is used to describe whether the
@c rows and columns of a distributed matrix or vector form a partition or
@c merely a cover of the rows and columns of a global matrix or
@c vector. In the case of a partition, each matrix or vector entry is
@c owned by a single MPI process. In the case of a cover, different MPI
@c processes are allowed to store values associated with the same matrix
@c or vector entry.
@c @example
@c @code{enum mtxpartitioning @{
@c     mtxpartition,   /* matrix/vector entries are owned
@c                          * by a single MPI process. */
@c     mtx_cover,       /* matrix/vector entries may be owned
@c                          * by multiple MPI processes. */
@c @};}
@c @end example
@c Note that some algorithms may only work with a partitioned matrix and
@c might produce incorrect results in the case of a covering. Thus, it
@c may be necessary to first perform a reduction to combine values
@c associated with matrix or vector entries that are distributed across
@c multiple MPI processes.


@c @node Index sets
@c @subsection Index sets

@c @cindex Index set
@c An @dfn{index set} is a set of integers, typically used to represent a
@c subset of the rows of a vector or the rows or columns of a
@c matrix. Index sets are used, for example, when specifying submatrices
@c of a matrix, or for partitioning and distributing matrices and vectors
@c among multiple processes.

@c @tindex struct mtxidxset
@c @tindex enum mtxidxsettype
@c The file @file{libmtx/util/index_set.h} defines data types for index
@c sets, including @code{struct mtxidxset}. There are different types
@c of index sets, which may be distinguished by the enum type
@c @code{mtxidxsettype}.
@c @itemize
@c @item @code{mtxidxset_interval}
@c represents an index set of contiguous integers from a half-open
@c interval @code{[a,b)}.

@c @item @code{mtxidxset_array}
@c represents a discrete index set, which is not necessarily contiguous,
@c as an array of integers.

@c @end itemize

@c An index set representing a half-open interval @code{[a,b)} can be
@c created with @code{mtxidxset_init_interval}.
@c @findex mtxidxset_init_interval
@c @example
@c @code{int mtxidxset_init_interval(
@c     struct mtxidxset * index_set, int a, int b);}
@c @end example
@c Then, the function @code{mtxidxset_contains} can be used to test if
@c a given integer @code{n} belongs to the index set.
@c @findex mtxidxset_contains
@c @example
@c @code{bool mtxidxset_contains(
@c     const struct mtxidxset * index_set, int n);}
@c @end example


@c @node Creating distributed matrices and vectors
@c @section Creating distributed matrices and vectors
@c A number of functions are provided to more conveniently construct
@c distributed matrices and vectors. These are described in the following
@c subsections.


@c @node mtxdist_free
@c @subsection mtxdist_free

@c @findex mtxdist_free
@c Since a distributed matrix or vector represented by a @code{struct
@c mtxdist} allocates some storage for its data, the user is required to
@c free the allocated storage by calling @code{mtxdist_free} when they
@c are finished with the matrix or vector:
@c @example
@c @code{void mtxdist_free(
@c     struct mtxdist * mtxdist);}
@c @end example


@c @node Creating distributed vectors
@c @subsection Creating distributed vectors


@c @node Creating distributed matrices
@c @subsection Creating distributed matrices
