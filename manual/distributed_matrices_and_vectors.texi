@c This file is part of Libmtx.
@c Copyright (C) 2022 James D. Trotter
@c
@c Libmtx is free software: you can redistribute it and/or modify it
@c under the terms of the GNU General Public License as published by
@c the Free Software Foundation, either version 3 of the License, or
@c (at your option) any later version.
@c
@c Libmtx is distributed in the hope that it will be useful, but
@c WITHOUT ANY WARRANTY; without even the implied warranty of
@c MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
@c General Public License for more details.
@c
@c You should have received a copy of the GNU General Public License
@c along with Libmtx.  If not, see <https://www.gnu.org/licenses/>.
@c
@c Authors: James D. Trotter <james@simula.no>
@c Last modified: 2022-10-03
@c
@c Libmtx User Guide: Distributed matrices and vectors.

@node Distributed matrices and vectors
@chapter Distributed matrices and vectors
This chapter describes how to distribute matrices and vectors across
multiple processes, and how to perform various linear algebra
operations on distributed matrices and vectors. The distributed-memory
computing features in Libmtx are implemented using MPI, and,
therefore, you will need to build Libmtx with MPI support. In many
cases, working with distributed matrices and vectors involves
converting to or from Matrix Market format, and it may therefore be a
good idea to read @ref{Distributed Matrix Market files} to understand
how to work with distributed Matrix Market files in Libmtx.

@menu
* Distributed vectors:: Data structures for distributed vectors.
* Distributed matrices:: Data structures for distributed matrices.
@end menu


@node Distributed vectors
@section Distributed vectors
@cindex distributed vector
@tindex struct mtxdistvector
The file @file{libmtx/linalg/mpi/vector.h} defines the type
@code{struct mtxdistvector}. This data type builds on top of
@code{struct mtxvector} (see @ref{Vectors}) to provide different
options for the underlying storage and implementation of vector
operations.

The definition of @code{struct mtxdistvector} is shown below.

@example
struct mtxdistvector @{
    MPI_Comm comm;
    int comm_size;
    int rank;
    struct mtxpartition rowpart;
    struct mtxvector interior;
@};
@end example

@noindent
Similar to @code{mtxdistfile} (see @ref{Data structures for
distributed Matrix Market files}), the first three struct members of
@code{mtxdistvector} contain information about the group of processes
sharing the distributed vector. This includes their MPI communicator
(@code{comm}), the number of processes (@code{comm_size}) and the rank
of the current process (@code{rank}). Also, @code{rowpart} defines a
partitioning of the vector elements. This is used to determine which
parts of the vector belongs to which process.

The final struct member, @code{interior}, stores the part of the
distributed vector that resides on the current process.


@node Creating distributed vectors
@subsection Creating distributed vectors
@cindex free
@findex mtxdistvector_free
This section describes how to create distributed vectors. But first,
the function @code{mtxdistvector_free} is used to free storage allocated
for a distributed vector.

@example
void mtxdistvector_free(struct mtxdistvector * distvector);
@end example

@cindex copy
@findex mtxdistvector_init_copy
To create a copy of an existing distributed vector, use the function
@code{mtxdistvector_init_copy}.

@example
int mtxdistvector_init_copy(
    struct mtxdistvector * dst,
    const struct mtxdistvector * src,
    struct mtxdisterror * disterr);
@end example

@cindex copy
@findex mtxdistvector_alloc_copy
If storage for a copy of an existing distributed vector is needed, but
the vector values should not be copied or initialised, use the
function @code{mtxdistvector_alloc_copy}.

@example
int mtxdistvector_alloc_copy(
    struct mtxdistvector * dst,
    const struct mtxdistvector * src,
    struct mtxdisterror * disterr);
@end example

@cindex allocate
@cindex array format
@cindex coordinate format
@findex mtxdistvector_alloc_array
@findex mtxdistvector_alloc_coordinate
To allocate storage for a distributed vector in @emph{array} or
@emph{coordinate} format, the functions
@code{mtxdistvector_alloc_array} or
@code{mtxdistvector_alloc_coordinate} may be used.

@example
int mtxdistvector_alloc_array(
    struct mtxdistvector * vector,
    enum mtxfield field,
    enum mtxprecision precision,
    int num_local_rows,
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);

int mtxdistvector_alloc_coordinate(
    struct mtxdistvector * vector,
    enum mtxfield field,
    enum mtxprecision precision,
    int num_local_rows,
    int64_t num_local_nonzeros,
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);
@end example

@noindent
In addition to the desired field and precision, the argument
@code{num_local_rows} is used to specify the number of rows of the
vector that are @emph{owned by the current process}. For a vector in
coordinate format, it is also necessary to specify the number of
nonzero vector elements (@code{num_local_nonzeros}) that will reside
on the current process. Note that the vector values are not
initialised, and so it is up to the user to initialise them.

If it is not @samp{NULL}, then @code{rowpart} must partition the rows
of the global vector. Consequently, if @code{rank} is the rank of the
current process, then @samp{rowpart->part_sizes[rank]} must be equal
to the value of @samp{num_local_rows}. Moreover, @samp{rowpart->size}
must be equal to the sum of the values of @samp{num_local_rows} on
every process in the communicator @code{comm}. The partition may have
at most one part for each MPI process in the communicator.

If @code{rowpart} is @samp{NULL}, then the rows of the vector are
partitioned into blocks with one block for each process in the MPI
communicator @code{comm}. Moreover, the size of each block is obtained
from the argument @code{num_local_rows} on each process.

@findex mtxdistvector_init_@var{type}_@var{field}_@var{precision}
If the vector values are already known, then there are also functions
for allocating a distributed vector and initialising the values
directly. This is done by calling
@code{mtxdistvector_init_@var{type}_@var{field}_@var{precision}},
where @code{@var{type}}, @code{@var{field}} and @code{@var{precision}}
denote the vector type (i.e., @samp{array} or @samp{coordinate}),
field (i.e., @samp{real}, @samp{complex} or @samp{integer}) and
precision (i.e., @samp{single} or @samp{double}).

@findex mtxdistvector_init_array_complex_double
For example, to create a double precision, complex vector in array
format, use @code{mtxdistvector_init_array_complex_double}.

@example
int mtxdistvector_init_array_complex_double(
    struct mtxdistvector * distvector,
    int num_local_rows,
    const double (* data)[2],
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);
@end example

@noindent
Each process provides its local vector entries in the array
@code{data}. The length of the @code{data} array must be at least
@samp{num_local_rows}. (There may be fewer parts in the partition than
MPI processes. In this case, the @code{data} array is not used on
processes where @samp{rank} is greater than or equal to
@samp{rowpart->num_parts}, and may therefore be set to @samp{NULL} on
those processes.)

@findex mtxdistvector_init_coordinate_complex_double
To create a double precision, complex vector in coordinate format, use
@code{mtxdistvector_init_coordinate_complex_double}.

@example
int mtxdistvector_init_coordinate_complex_double(
    struct mtxdistvector * vector,
    int num_local_rows,
    int64_t num_local_nonzeros,
    const int * idx,
    const double (* data)[2],
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);
@end example

@noindent
The arguments @code{idx} and @code{data} are arrays of length
@code{num_local_nonzeros}. Each process may provide arrays of
different lengths. Each index @samp{idx[0]}, @samp{idx[1]}, ...,
@samp{idx[num_local_nonzeros-1]}, is an integer in the range
@samp{[0,N)}, where @samp{N} is the size of the part owned by the
current process, (i.e., @samp{rowpart->part_sizes[rank]}, where
@samp{rank} is the rank of the current process).

Note that duplicate entries are allowed, but this may cause some
operations (e.g., @code{mtxdistvector_[sd]dot},
@code{mtxdistvector_[sd]nrm2})) to produce incorrect results.


@node Modifying values of distributed vectors
@subsection Modifying values
@findex mtxdistvector_set_constant_@var{field}_@var{precision}
The functions
@code{mtxdistvector_set_constant_@var{field}_@var{precision}} can be
used to set every (nonzero) value of a vector equal to a constant
scalar, where @code{@var{field}} and @code{@var{precision}} should
match the field (i.e., @samp{real}, @samp{complex} or @samp{integer})
and precision (i.e., @samp{single} or @samp{double}) of
@code{mtxdistvector}.
@findex mtxdistvector_set_constant_@var{field}_@var{precision}
@findex mtxdistvector_set_constant_real_single
@findex mtxdistvector_set_constant_real_double
@findex mtxdistvector_set_constant_complex_single
@findex mtxdistvector_set_constant_complex_double
@findex mtxdistvector_set_constant_integer_single
@findex mtxdistvector_set_constant_integer_double

@example
int mtxdistvector_set_constant_real_single(
    struct mtxdistvector *, float a, struct mtxdisterror * disterr);
int mtxdistvector_set_constant_real_double(
    struct mtxdistvector *, double a, struct mtxdisterror * disterr);
int mtxdistvector_set_constant_complex_single(
    struct mtxdistvector *, float a[2], struct mtxdisterror * disterr);
int mtxdistvector_set_constant_complex_double(
    struct mtxdistvector *, double a[2], struct mtxdisterror * disterr);
int mtxdistvector_set_constant_integer_single(
    struct mtxdistvector *, int32_t a, struct mtxdisterror * disterr);
int mtxdistvector_set_constant_integer_double(
    struct mtxdistvector *, int64_t a, struct mtxdisterror * disterr);
@end example

To access or modify individual vector elements, the underlying vector
storage is accessed through the appropriate member of the
@code{storage} union in the @code{mtxvector} struct.


@node Converting distributed vectors to and from Matrix Market format
@subsection Converting to and from Matrix Market format
@cindex convert to and from Matrix Market format
@cindex convert to and from distributed Matrix Market format
A distributed vector can be obtained from a Matrix Market file by
distributing the Matrix Market entries across multiple processes
before converting the data on each process to the desired vector
storage format. Typically, this involves partitioning the rows of the
vector and distributing the data accordingly. If the Matrix Market
file is already distributed across several processes, then the data is
partitioned and redistributed before converting to the desired vector
storage format.

Conversely, a distributed vector can be converted directly to
distributed Matrix Market format without the need for redistributing
any data. If desirable, the data may also be gathered onto a single,
root process after converting to Matrix Market format. In either case,
converting to Matrix Market format allows the data to be easily
written to a Matrix Market file.

@findex mtxdistvector_from_mtxfile
To convert a vector in Matrix Market format to @code{struct
mtxdistvector}, the function @code{mtxdistvector_from_mtxfile} can be
used. In this case, the Matrix Market file @code{mtxfile} must reside
on the process whose rank is @code{root}.

@example
int mtxdistvector_from_mtxfile(
    struct mtxdistvector * dst,
    const struct mtxfile * src,
    enum mtxvectortype type,
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    int root,
    struct mtxdisterror * disterr);
@end example

@noindent
The @code{type} argument may be used to specify a desired storage
format or implementation for the underlying @code{mtxvector} on each
process. If @code{type} is @samp{mtxvector_auto}, then the type of
@code{mtxvector} is chosen to match the type of @code{src}. That is,
@samp{mtxvector_array} is used if @code{src} is in array format, and
@samp{mtxvector_coordinate} is used if @code{src} is in coordinate
format.

Furthermore, @code{rowpart} must be a partitioning of the rows of the
global vector. Therefore, @code{rowpart->size} must be equal to the
number of rows in the underlying vector represented by
@code{mtxfile}. The partition must consist of at most one part for
each MPI process in the communicator @code{comm}. If @code{rowpart} is
@samp{NULL}, then the rows are partitioned into contiguous blocks of
equal size by default.


@findex mtxdistvector_to_mtxfile
To convert @code{struct mtxdistvector} back to Matrix Market format,
the function @code{mtxdistvector_to_mtxfile} can be used.

@example
int mtxdistvector_to_mtxfile(
    const struct mtxdistvector * mtxdistvector,
    struct mtxfile * mtxfile,
    enum mtxfileformat mtxfmt,
    MPI_Comm comm,
    int root,
    struct mtxdisterror * disterr);
@end example

@noindent
The resulting Matrix Market file resides on the process whose rank is
@code{root}. The vector is stored in array format if @code{mtxfmt} is
@samp{mtxfile_array} or in coordinate format if @code{mtxfmt} is
@samp{mtxfile_coordinate}.


@findex mtxdistvector_from_mtxdistfile
If a Matrix Market file has already been distributed among multiple
processes, then @code{mtxdistvector_from_mtxdistfile} can be used to
obtain a distributed vector with the desired partitioning and storage
format.

@example
int mtxdistvector_from_mtxdistfile(
    struct mtxdistvector * distvector,
    const struct mtxdistfile * mtxdistfile,
    enum mtxvectortype type,
    const struct mtxpartition * rowpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);
@end example

@noindent
Each process partitions its part of the distributed Matrix Market
file. The data is then redistributed after partitioning.

@findex mtxdistvector_to_mtxdistfile
The function @code{mtxdistvector_to_mtxdistfile} will convert a
distributed vector to a distributed Matrix Market format.

@example
int mtxdistvector_to_mtxdistfile(
    struct mtxdistfile * dst,
    const struct mtxdistvector * src,
    enum mtxfileformat mtxfmt,
    struct mtxdisterror * disterr);
@end example

@noindent
In this case, there is no redistribution or communication of the
underlying data.

@node Reading and writing distributed vectors
@subsection Reading and writing Matrix Market files
@cindex file I/O
@cindex reading files
Distributed vectors can be read from or written to files in Matrix
Market format, much in the same way as described for vectors on a
single process in @ref{Reading and writing vectors}. Therefore, the
functions @code{mtxdistvector_read}, @code{mtxdistvector_fread} and
@code{mtxdistvector_gzread} are provided to easily read a vector from
a file in Matrix Market format and distribute it among a group of
processes, before converting it to a desired vector representation on
each process.
@findex mtxdistvector_read
@findex mtxdistvector_fread
@findex mtxdistvector_gzead

@example
int mtxdistvector_read(
    struct mtxdistvector * vector,
    enum mtxprecision precision,
    enum mtxvectortype type,
    const char * path,
    bool gzip,
    int * lines_read,
    int64_t * bytes_read);

int mtxdistvector_fread(
    struct mtxdistvector * vector,
    enum mtxprecision precision,
    enum mtxvectortype type,
    FILE * f,
    int * lines_read,
    int64_t * bytes_read,
    size_t line_max,
    char * linebuf);

int mtxdistvector_gzread(
    struct mtxdistvector * vector,
    enum mtxprecision precision,
    enum mtxvectortype type,
    gzFile f,
    int * lines_read,
    int64_t * bytes_read,
    size_t line_max,
    char * linebuf);
@end example

@noindent
Here @code{type} specifies a format to use for representing the
vector, whereas @code{precision} specifies the precision to use for
representing individual vector values. If @code{type} is
@samp{mtxvector_auto}, then the underlying vector is stored in array
format or coordinate format, depending on the format of the Matrix
Market file. Otherwise, an attempt is made to convert the vector to
the desired type. The remaining arguments are used in the same way as
described in @ref{Reading Matrix Market files}.

@cindex writing files
@findex mtxdistvector_write
@findex mtxdistvector_fwrite
@findex mtxdistvector_gzwrite
Conversely, the functions @code{mtxdistvector_write},
@code{mtxdistvector_fwrite} and @code{mtxdistvector_gzwrite} are
provided to write a vector to a file in Matrix Market format.

@example
int mtxdistvector_write(
    const struct mtxdistvector * vector,
    enum mtxfileformat mtxfmt,
    const char * path,
    bool gzip,
    const char * fmt,
    int64_t * bytes_written);

int mtxdistvector_fwrite(
    const struct mtxdistvector * vector,
    enum mtxfileformat mtxfmt,
    FILE * f,
    const char * fmt,
    int64_t * bytes_written);

int mtxdistvector_gzwrite(
    const struct mtxdistvector * vector,
    enum mtxfileformat mtxfmt,
    gzFile f,
    const char * fmt,
    int64_t * bytes_written);
@end example

@noindent
The @code{mtxfmt} argument may be used to specify whether the vector
should be written in array or coordinate format.

@node Level 1 BLAS for distributed vectors
@subsection Level 1 BLAS
@cindex BLAS
The same BLAS routines that were described in @ref{Level 1 BLAS for
vectors} are also available for distributed vectors. The main
difference is that each function takes an extra argument of type
@code{struct mtxdisterror} to allow for robust error handling in a
distributed setting (see @ref{Distributed error handling}). In
addition, some of the level 1 BLAS routines require processes to
communicate with one another to produce the correct result. In
particular, computing dot products and norms typically require a
reduction (e.g., @code{MPI_Allreduce}) among all processes involved.

This section briefly describes the level 1 BLAS functions for
distributed vectors.

@findex mtxdistvector_swap
@findex mtxdistvector_copy
The function @code{mtxdistvector_swap} swaps the values of two
vectors, whereas @code{mtxdistvector_copy} copies the values from one
vector to another. Both operations are performed without any
communication.

@example
int mtxdistvector_swap(
    struct mtxdistvector * x,
    struct mtxdistvector * y,
    struct mtxdisterror * disterr);

int mtxdistvector_copy(
    struct mtxdistvector * y,
    const struct mtxdistvector * x,
    struct mtxdisterror * disterr);
@end example

@findex mtxdistvector_sscal
@findex mtxdistvector_dscal
The functions @code{mtxdistvector_sscal} and
@code{mtxdistvector_dscal} scale a vector @code{x} by a floating point
constant @code{a} in single or double precision, respectively. That
is, @code{x = a*x}. This operation does not require communication
between processes.

@example
int mtxdistvector_sscal(
    float a,
    struct mtxdistvector * x,
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_dscal(
    double a,
    struct mtxdistvector * x,
    int64_t * num_flops,
    struct mtxdisterror * disterr);
@end example

@noindent
If @code{num_flops} is not @samp{NULL}, then it is used to return the
total number of floating point operations performed by all processes.

@findex mtxdistvector_saxpy
@findex mtxdistvector_daxpy
@findex mtxdistvector_saypx
@findex mtxdistvector_daypx
The functions @code{mtxdistvector_saxpy}, @code{mtxdistvector_daxpy},
@code{mtxdistvector_saypx} and @code{mtxdistvector_daypx} add together
vectors multiplied by a single or double precision floating point
value, @code{y = a*x + y} or @code{y = a*y + x}. This may also be done
without needing to perform communication.

@example
int mtxdistvector_saxpy(
    float a,
    const struct mtxdistvector * x,
    struct mtxdistvector * y,
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_daxpy(
    double a,
    const struct mtxdistvector * x,
    struct mtxdistvector * y,
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_saypx(
    float a,
    struct mtxdistvector * y,
    const struct mtxdistvector * x,
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_daypx(
    double a,
    struct mtxdistvector * y,
    const struct mtxdistvector * x,
    int64_t * num_flops,
    struct mtxdisterror * disterr);
@end example

@findex mtxdistvector_sdot
@findex mtxdistvector_ddot
The functions @code{mtxdistvector_sdot} and @code{mtxdistvector_ddot}
compute the Euclidean dot product of two real- or integer-valued
vectors. This performs a reduction among all processes involved to
produce the final result.

@example
int mtxdistvector_sdot(
    const struct mtxdistvector * x,
    const struct mtxdistvector * y,
    float * dot,
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_ddot(
    const struct mtxdistvector * x,
    const struct mtxdistvector * y,
    double * dot,
    int64_t * num_flops,
    struct mtxdisterror * disterr);
@end example

@findex mtxdistvector_cdotu
@findex mtxdistvector_zdotu
@findex mtxdistvector_cdotc
@findex mtxdistvector_zdotc
For complex vectors, the functions @code{mtxdistvector_cdotu} and
@code{mtxdistvector_zdotu} compute the product of the transpose of a
complex row vector with another complex row vector, @code{x^T*y},
where @code{x^T} denotes the transpose of @code{x}. The functions
@code{mtxdistvector_cdotc} and @code{mtxdistvector_zdotc} compute the
Euclidean dot product of two complex vectors, @code{x^H*y}, where
@code{x^H} denotes the conjugate transpose of @code{x}.

@example
int mtxdistvector_cdotu(
    const struct mtxdistvector * x,
    const struct mtxdistvector * y,
    float (* dot)[2],
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_zdotu(
    const struct mtxdistvector * x,
    const struct mtxdistvector * y,
    double (* dot)[2],
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_cdotc(
    const struct mtxdistvector * x,
    const struct mtxdistvector * y,
    float (* dot)[2],
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_zdotc(
    const struct mtxdistvector * x,
    const struct mtxdistvector * y,
    double (* dot)[2],
    int64_t * num_flops,
    struct mtxdisterror * disterr);
@end example

@findex mtxdistvector_snrm2
@findex mtxdistvector_dnrm2
The functions @code{mtxdistvector_snrm2} and
@code{mtxdistvector_dnrm2} compute the Euclidean norm of a vector. in
single and double precision floating point, respectively.

@example
int mtxdistvector_snrm2(
    const struct mtxdistvector * x,
    float * nrm2,
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_dnrm2(
    const struct mtxdistvector * x,
    double * nrm2,
    int64_t * num_flops,
    struct mtxdisterror * disterr);
@end example

@findex mtxdistvector_sasum
@findex mtxdistvector_dasum
The functions @code{mtxdistvector_sasum} and
@code{mtxdistvector_dasum} compute the sum of absolute values, or
1-norm, of a vector. in single and double precision floating point,
respectively. If the vector is complex-valued, then the sum of the
absolute values of the real and imaginary parts is computed. A
reduction is performed to combine the values computed by each process.

@example
int mtxdistvector_sasum(
    const struct mtxdistvector * x,
    float * asum,
    int64_t * num_flops,
    struct mtxdisterror * disterr);

int mtxdistvector_dasum(
    const struct mtxdistvector * x,
    double * asum,
    int64_t * num_flops,
    struct mtxdisterror * disterr);
@end example

@findex mtxdistvector_iamax
The function @code{mtxdistvector_iamax} finds the index of the first
element having the largest absolute value among all the vector
elements. If the vector is complex-valued, then the index points to
the first element having the maximum sum of the absolute values of the
real and imaginary parts.

@example
int mtxdistvector_iamax(
    const struct mtxdistvector * x,
    int * iamax,
    struct mtxdisterror * disterr);
@end example

@node Halo updates
@subsection Halo updates
@cindex halo update
Some linear algebra operations, such as matrix-vector multiplication,
require communication between MPI processes whenever distributed
matrices and vectors are used. This kind of communication is commonly
implemented through a @dfn{halo update}. This section describes data
structures for representing vector halos, and how to perform halo
updates.

@example
int mtxdistvector_halo_update(
    struct mtxdistvector * dst,
    const struct mtxdistvector * src,
    struct mtxdisterror * disterr);
@end example


@node Distributed matrices
@section Distributed matrices
@cindex distributed matrix
@tindex struct mtxdistmatrix
The file @file{libmtx/linalg/mpi/matrix.h} defines the type
@code{struct mtxdistmatrix}. This data type builds on top of
@code{struct mtxmatrix} (see @ref{Matrices}) to offer different
options for the underlying storage and implementation of matrix
operations.

The definition of @code{struct mtxdistmatrix} is shown below.

@example
struct mtxdistmatrix @{
    MPI_Comm comm;
    int comm_size;
    int rank;
    struct mtxpartition rowpart;
    struct mtxpartition colpart;
    struct mtxmatrix interior;
@};
@end example

@noindent
This is mostly similar to @code{struct mtxdistvector}, except that
@code{interior} is of type @code{struct mtxmatrix} and is used to
store the part of the distributed matrix that resides on the current
process. In addition, there is an additional struct member,
@code{colpart}, to partition the matrix columns. Libmtx thus allows 2D
partitionings and distributions of matrices among a two-dimensional
grid of processes.

@node Creating distributed matrices
@subsection Creating distributed matrices
@cindex free
@findex mtxdistmatrix_free
This section describes how to create distributed matrices. But first,
the function @code{mtxdistmatrix_free} is used to free storage allocated
for a distributed matrix.

@example
void mtxdistmatrix_free(struct mtxdistmatrix * distmatrix);
@end example

@cindex copy
@findex mtxdistmatrix_init_copy
To create a copy of an existing distributed matrix, use the function
@code{mtxdistmatrix_init_copy}.

@example
int mtxdistmatrix_init_copy(
    struct mtxdistmatrix * dst,
    const struct mtxdistmatrix * src,
    struct mtxdisterror * disterr);
@end example

@cindex copy
@findex mtxdistmatrix_alloc_copy
If storage for a copy of an existing distributed matrix is needed, but
the matrix values should not be copied or initialised, use the
function @code{mtxdistmatrix_alloc_copy}.

@example
int mtxdistmatrix_alloc_copy(
    struct mtxdistmatrix * dst,
    const struct mtxdistmatrix * src,
    struct mtxdisterror * disterr);
@end example

@cindex allocate
@cindex array format
@cindex coordinate format
@findex mtxdistmatrix_alloc_array
@findex mtxdistmatrix_alloc_coordinate
To allocate storage for a distributed matrix in @emph{array} or
@emph{coordinate} format, the functions
@code{mtxdistmatrix_alloc_array} or
@code{mtxdistmatrix_alloc_coordinate} may be used.

@example
int mtxdistmatrix_alloc_array(
    struct mtxdistmatrix * matrix,
    enum mtxfield field,
    enum mtxprecision precision,
    int num_local_rows,
    int num_local_columns,
    const struct mtxpartition * rowpart,
    const struct mtxpartition * colpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);

int mtxdistmatrix_alloc_coordinate(
    struct mtxdistmatrix * matrix,
    enum mtxfield field,
    enum mtxprecision precision,
    int num_local_rows,
    int num_local_columns,
    int64_t num_local_nonzeros,
    const struct mtxpartition * rowpart,
    const struct mtxpartition * colpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);
@end example

@noindent
Each process is responsible for a rectangular block of size
@code{num_local_rows} and @code{num_local_columns} defined by the
intersection certain rows and columns of the global matrix. For a
matrix in coordinate format, it is also necessary to specify the
number of nonzero matrix elements (@code{num_local_nonzeros}) that
will reside on the current process. Note that matrix values are not
initialised, and so it is up to the user to initialise them.

If it is not @samp{NULL}, then @code{rowpart} must partition the rows
of the global matrix. Consequently, if @code{rank} is the rank of the
current process, then @samp{rowpart->part_sizes[rank]} must be equal
to the value of @samp{num_local_rows}. Moreover, @samp{rowpart->size}
must be equal to the sum of the values of @samp{num_local_rows} on
every process in the communicator @code{comm}. The partition may have
at most one part for each MPI process in the communicator. In the same
manner, @code{colpart} may be used to partition the matrix columns.

If @code{rowpart} and @code{colpart} are both @samp{NULL}, then the
rows of the matrix are partitioned into blocks with one block for each
process in the MPI communicator @code{comm}. Moreover, the size of
each block is obtained from the argument @code{num_local_rows} on each
process. If only one of @code{rowpart} or @code{colpart} are
@samp{NULL}, then a singleton partition is used for the rows or
columns, respectively.

@findex mtxdistmatrix_init_@var{type}_@var{field}_@var{precision}
If the matrix values are already known, then there are also functions
for allocating a distributed matrix and initialising the values
directly. This is done by calling
@code{mtxdistmatrix_init_@var{type}_@var{field}_@var{precision}},
where @code{@var{type}}, @code{@var{field}} and @code{@var{precision}}
denote the matrix type (i.e., @samp{array} or @samp{coordinate}),
field (i.e., @samp{real}, @samp{complex} or @samp{integer}) and
precision (i.e., @samp{single} or @samp{double}).

@findex mtxdistmatrix_init_array_complex_double
For example, to create a double precision, complex matrix in array
format, use @code{mtxdistmatrix_init_array_complex_double}.

@example
int mtxdistmatrix_init_array_complex_double(
    struct mtxdistmatrix * distmatrix,
    int num_local_rows,
    int num_local_columns,
    const double (* data)[2],
    const struct mtxpartition * rowpart,
    const struct mtxpartition * colpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);
@end example

@noindent
Each process provides its local matrix entries in the array
@code{data}. The length of the @code{data} array must be at least
@iftex
@tex${\tt num\_local\_rows}\times {\tt num\_local\_columns}$. @end tex
@end iftex
@ifnottex
@math{@t{num_local_rows} times @t{num_local_columns}}.
@end ifnottex
There may be fewer parts in the partition than MPI processes. In this
case, the @code{data} array is not used on processes where @samp{rank}
is greater than or equal to @samp{rowpart->num_parts}, and may
therefore be set to @samp{NULL} on those processes.

@findex mtxdistmatrix_init_coordinate_complex_double
To create a double precision, complex matrix in coordinate format, use
@code{mtxdistmatrix_init_coordinate_complex_double}.

@example
int mtxdistmatrix_init_coordinate_complex_double(
    struct mtxdistmatrix * matrix,
    int num_local_rows,
    int num_local_columns,
    int64_t num_local_nonzeros,
    const int * rowidx,
    const int * colidx,
    const double (* data)[2],
    const struct mtxpartition * rowpart,
    const struct mtxpartition * colpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);
@end example

@noindent
The arguments @code{rowidx}, @code{colidx} and @code{data} are arrays
of length @code{num_local_nonzeros}. Thus, each process may provide
arrays of different lengths. Each row index @samp{rowidx[0]},
@samp{rowidx[1]}, ..., @samp{rowidx[num_local_nonzeros-1]}, is an
integer in the range @samp{[0,M)}, where @samp{M} is the number of
rows owned by the current process, (i.e.,
@samp{rowpart->part_sizes[rank]}, where @samp{rank} is the rank of the
current process). Similarly, each column index @samp{colidx[0]},
@samp{colidx[1]}, ..., @samp{colidx[num_local_nonzeros-1]}, is an
integer in the range @samp{[0,N)}, where @samp{N} is the number of
columns owned by the current process.

Note that duplicate entries are allowed, but this may cause some
operations (e.g., @code{mtxdistmatrix_[sd]dot},
@code{mtxdistmatrix_[sd]nrm2} and @code{mtxdistmatrix_[sdcz]gemv}) to
produce incorrect results.


@node Creating row and column vectors for distributed matrices
@subsection Creating row and column vectors

@cindex row vector
@cindex column vector
Row and column vectors can also be created for distributed matrices,
though this requires some additional considerations compared to the
non-distributed case described in @ref{Creating row and column
vectors}. More specifically, the allocated vectors are created so that
they are compatible with the partitioning of the distributed matrix. A
row vector is thus partitioned in the same way as the matrix columns,
whereas a column vector is partitioned in the same way as the matrix
rows.

@findex mtxdistmatrix_alloc_row_vector
@findex mtxdistmatrix_alloc_column_vector
To create distributed row and column vectors, the functions
@code{mtxdistmatrix_alloc_row_vector} and
@code{mtxdistmatrix_alloc_column_vector} may be used.

@example
int mtxdistmatrix_alloc_row_vector(
    const struct mtxdistmatrix * distmatrix,
    struct mtxdistvector * distvector,
    enum mtxvectortype vector_type,
    struct mtxdisterror * disterr)

int mtxdistmatrix_alloc_column_vector(
    const struct mtxdistmatrix * distmatrix,
    struct mtxdistvector * distvector,
    enum mtxvectortype vector_type,
    struct mtxdisterror * disterr)
@end example

@noindent
The argument @code{vector_type} is used to specify the desired,
underlying storage type for the row or column vector.


@node Converting distributed matrices to and from Matrix Market format
@subsection Converting to and from Matrix Market format
@cindex convert to and from Matrix Market format
@cindex convert to and from distributed Matrix Market format
Given a Matrix Market file that represents a matrix can be converted
to a distributed matrix by partitioning and distributing the Matrix
Market entries among processes, before converting the data on each
process to the desired matrix storage format. A two-dimensional
partitioning is obtained by partitioning the matrix rows and columns,
and matrix entries are distributed accordingly. If the Matrix Market
file is already distributed across several processes, then the data is
partitioned and redistributed before converting to the desired matrix
storage format.

Conversely, a distributed matrix can be converted directly to
distributed Matrix Market format without the need for redistributing
any data. If desirable, the data may also be gathered onto a single,
root process after converting to Matrix Market format. In any case,
converting to Matrix Market format allows the data to be easily
written to a Matrix Market file.

@findex mtxdistmatrix_from_mtxfile
To convert a matrix in Matrix Market format to @code{struct
mtxdistmatrix}, the function @code{mtxdistmatrix_from_mtxfile} can be
used. In this case, the Matrix Market file @code{mtxfile} must reside
on the process whose rank is @code{root}.

@example
int mtxdistmatrix_from_mtxfile(
    struct mtxdistmatrix * dst,
    const struct mtxfile * src,
    enum mtxmatrixtype type,
    const struct mtxpartition * rowpart,
    const struct mtxpartition * colpart,
    MPI_Comm comm,
    int root,
    struct mtxdisterror * disterr);
@end example

@noindent
The @code{type} argument may be used to specify a desired storage
format or implementation for the underlying @code{mtxmatrix} on each
process. If @code{type} is @samp{mtxmatrix_auto}, then the type of
@code{mtxmatrix} is chosen to match the type of @code{src}. That is,
@samp{mtxmatrix_array} is used if @code{src} is in array format, and
@samp{mtxbasecoo} is used if @code{src} is in coordinate
format.

Furthermore, @code{rowpart} and @code{colpart} must be partitionings
of the rows and columns of the matrix, respectively. Therefore,
@samp{rowpart->size} must be equal to the number of rows and
@samp{colpart->size} must be equal to the number of columns in the
underlying matrix represented by @code{mtxfile}. The number of parts
in the 2D partitioning of the matrix is equal to the product of number
of parts in the row and column partitions. There may be at most one
part for each MPI process in the communicator @code{comm}. If
@code{rowpart} and @code{colparts} are both @samp{NULL}, then the rows
are partitioned into contiguous blocks of equal size by default.

@findex mtxdistmatrix_to_mtxfile
To convert @code{struct mtxdistmatrix} back to Matrix Market format,
the function @code{mtxdistmatrix_to_mtxfile} can be used.

@example
int mtxdistmatrix_to_mtxfile(
    const struct mtxdistmatrix * mtxdistmatrix,
    struct mtxfile * mtxfile,
    enum mtxfileformat mtxfmt,
    MPI_Comm comm,
    int root,
    struct mtxdisterror * disterr);
@end example

@noindent
The resulting Matrix Market file resides on the process whose rank is
@code{root}. The matrix is stored in array format if @code{mtxfmt} is
@samp{mtxfile_array} or in coordinate format if @code{mtxfmt} is
@samp{mtxfile_coordinate}.

@findex mtxdistmatrix_from_mtxdistfile
If a Matrix Market file has already been distributed among multiple
processes, then @code{mtxdistmatrix_from_mtxdistfile} can be used to
obtain a distributed matrix with the desired partitioning and storage
format.

@example
int mtxdistmatrix_from_mtxdistfile(
    struct mtxdistmatrix * distmatrix,
    const struct mtxdistfile * mtxdistfile,
    enum mtxmatrixtype type,
    const struct mtxpartition * rowpart,
    const struct mtxpartition * colpart,
    MPI_Comm comm,
    struct mtxdisterror * disterr);
@end example

@noindent
Each process partitions its part of the distributed Matrix Market
file. The data is then redistributed after partitioning.

@findex mtxdistmatrix_to_mtxdistfile
The function @code{mtxdistmatrix_to_mtxdistfile} will convert a
distributed matrix to a distributed Matrix Market format.

@example
int mtxdistmatrix_to_mtxdistfile(
    struct mtxdistfile * dst,
    const struct mtxdistmatrix * src,
    enum mtxfileformat mtxfmt,
    struct mtxdisterror * disterr);
@end example

@noindent
In this case, there is no redistribution or communication of the
underlying data.

@node Level 1 BLAS for distributed matrices
@subsection Level 1 BLAS
@cindex BLAS
Level 1 BLAS operations can also be applied to distributed matrices,
and this works exactly as described for distributed vectors in
@ref{Level 1 BLAS for distributed vectors}. Recall that for matrices,
the dot product and 2-norm become the Frobenius inner product and
Frobenius norm instead.

@node Level 2 BLAS for distributed matrices
@subsection Level 2 BLAS
@cindex BLAS
The Level 2 BLAS operations encompass matrix-vector operations, such
as matrix-vector multiplication. This section describes how to apply
these operations to distributed matrices and vectors.

@cindex matrix-vector multiplication
@cindex gemv
The function @code{mtxdistmatrix_sgemv} multiplies a matrix @code{A}
or its transpose @samp{A'} by a real scalar @code{alpha} (@code{α})
and a vector @code{x}, before adding the result to another vector
@code{y} multiplied by another real scalar @code{beta}
(@code{β}). That is, @code{y = α*A*x + β*y} or @code{y = α*A'*x +
β*y}. In this version, the scalars @code{alpha} and @code{beta} are
given as single precision floating point numbers.

@example
int mtxdistmatrix_sgemv(
    enum mtxtransposition trans,
    float alpha,
    const struct mtxdistmatrix * A,
    const struct mtxdistvector * x,
    float beta,
    struct mtxdistvector * y);
@end example

@noindent
If @code{trans} is @samp{mtx_notrans}, the matrix @code{A} is used. If
@code{trans} is @samp{mtx_trans}, then @code{A'} is used instead.

The function @code{mtxdistmatrix_dgemv} performs the same operation as
@code{mtxdistmatrix_sgemv}, except that the scalars @code{alpha} and
@code{beta} are now given as double precision floating point numbers.

@example
int mtxdistmatrix_dgemv(
    enum mtxtransposition trans,
    double alpha,
    const struct mtxdistmatrix * A,
    const struct mtxdistvector * x,
    double beta,
    struct mtxdistvector * y);
@end example

There are also two analogous routines, @code{mtxdistmatrix_cgemv} and
@code{mtxdistmatrix_zgemv} for the cases where @code{alpha} and
@code{beta} are given as complex numbers in single and double
precision floating point, respectively. These functions can also be
used to multiply with the conjugate transpose @samp{A^H}, if
@code{trans} is @samp{mtx_conjtrans}.

@example
int mtxdistmatrix_cgemv(
    enum mtxtransposition trans,
    float alpha[2],
    const struct mtxdistmatrix * A,
    const struct mtxdistvector * x,
    float beta[2],
    struct mtxdistvector * y);

int mtxdistmatrix_zgemv(
    enum mtxtransposition trans,
    double alpha[2],
    const struct mtxdistmatrix * A,
    const struct mtxdistvector * x,
    double beta[2],
    struct mtxdistvector * y);
@end example

@c @node Data distribution
@c @subsection Data distribution

@c @cindex data distribution
@c @cindex distributed matrix
@c @cindex distributed vector
@c @cindex block distribution
@c @cindex cyclic distribution
@c @cindex block-cyclic distribution
@c @cindex discrete distribution
@c @tindex mtx_distribution
@c It is often necessary to distribute large matrices and vectors across
@c multiple processes, both for the purpose of performing computations in
@c parallel and also to use multiple nodes, thereby increasing the total
@c amount of available memory.  To facilitate such data distribution,
@c some additional information is stored in the @code{mtx} struct.

@c First, we define the additional enum type @code{mtx_distribution},
@c which describes different methods for distributing a one-dimensional
@c data structure, such as a vector, among multiple processes.  Matrices
@c are distributed by independently specifying the distributions of the
@c rows and columns.
@c @example
@c @code{enum mtx_distribution @{
@c     mtx_private,           /* owned by a single process */
@c     mtx_replicated,        /* replicated across every process */
@c     mtx_block,             /* block distribution */
@c     mtx_cyclic,            /* cyclic distribution */
@c     mtx_block_cyclic,      /* block-cyclic distribution */
@c     mtx_discrete,          /* discrete distribution */
@c @};}
@c @end example
@c By default, matrices and vectors are not distributed
@c (@code{mtx_private}).  That is, the entries of a vector and the rows
@c and columns of a matrix are owned by a single process.

@c For a distributed vector, @code{mtx_block} is used when the vector is
@c partitioned into contiguous blocks of roughly equal size and one block
@c is assigned to each process.  In contrast, @code{mtx_cyclic} assigns
@c consecutive entries of the vector to successive processes.  By
@c generalising the block and cyclic distributions,
@c @code{mtx_block_cyclic} assigns consecutive, fixed-size blocks to
@c successive processes.  Finally, @code{mtx_discrete} allows an
@c arbitrary assignment of global vector entries to processes.


@c @cindex cover
@c @cindex partition
@c @tindex mtxpartitioning
@c The enum type @code{mtxpartitioning}, is used to describe whether the
@c rows and columns of a distributed matrix or vector form a partition or
@c merely a cover of the rows and columns of a global matrix or
@c vector. In the case of a partition, each matrix or vector entry is
@c owned by a single MPI process. In the case of a cover, different MPI
@c processes are allowed to store values associated with the same matrix
@c or vector entry.
@c @example
@c @code{enum mtxpartitioning @{
@c     mtxpartition,   /* matrix/vector entries are owned
@c                          * by a single MPI process. */
@c     mtx_cover,       /* matrix/vector entries may be owned
@c                          * by multiple MPI processes. */
@c @};}
@c @end example
@c Note that some algorithms may only work with a partitioned matrix and
@c might produce incorrect results in the case of a covering. Thus, it
@c may be necessary to first perform a reduction to combine values
@c associated with matrix or vector entries that are distributed across
@c multiple MPI processes.


@c @node Index sets
@c @subsection Index sets

@c @cindex Index set
@c An @dfn{index set} is a set of integers, typically used to represent a
@c subset of the rows of a vector or the rows or columns of a
@c matrix. Index sets are used, for example, when specifying submatrices
@c of a matrix, or for partitioning and distributing matrices and vectors
@c among multiple processes.

@c @tindex struct mtxidxset
@c @tindex enum mtxidxsettype
@c The file @file{libmtx/util/index_set.h} defines data types for index
@c sets, including @code{struct mtxidxset}. There are different types
@c of index sets, which may be distinguished by the enum type
@c @code{mtxidxsettype}.
@c @itemize
@c @item @code{mtxidxset_interval}
@c represents an index set of contiguous integers from a half-open
@c interval @code{[a,b)}.

@c @item @code{mtxidxset_array}
@c represents a discrete index set, which is not necessarily contiguous,
@c as an array of integers.

@c @end itemize

@c An index set representing a half-open interval @code{[a,b)} can be
@c created with @code{mtxidxset_init_interval}.
@c @findex mtxidxset_init_interval
@c @example
@c @code{int mtxidxset_init_interval(
@c     struct mtxidxset * index_set, int a, int b);}
@c @end example
@c Then, the function @code{mtxidxset_contains} can be used to test if
@c a given integer @code{n} belongs to the index set.
@c @findex mtxidxset_contains
@c @example
@c @code{bool mtxidxset_contains(
@c     const struct mtxidxset * index_set, int n);}
@c @end example
