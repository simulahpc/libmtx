@c This file is part of libmtx.
@c Copyright (C) 2022 James D. Trotter
@c
@c libmtx is free software: you can redistribute it and/or modify it
@c under the terms of the GNU General Public License as published by
@c the Free Software Foundation, either version 3 of the License, or
@c (at your option) any later version.
@c
@c libmtx is distributed in the hope that it will be useful, but
@c WITHOUT ANY WARRANTY; without even the implied warranty of
@c MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
@c General Public License for more details.
@c
@c You should have received a copy of the GNU General Public License
@c along with libmtx.  If not, see <https://www.gnu.org/licenses/>.
@c
@c Authors: James D. Trotter <james@simula.no>
@c Last modified: 2022-01-07
@c
@c libmtx User Guide: Distributed Matrix Market files.

@node Distributed Matrix Market files
@chapter Distributed Matrix Market files

This chapter describes how to distribute Matrix Market files among
multiple processes using MPI, and how to perform various operations on
those files in a distributed manner. The distributed-memory computing
features in libmtx are implemented using MPI, and you will therefore
need to build libmtx with MPI support.

For most user-facing types and functions that relate to distributed
computing, libmtx uses the convention of prefixing their names with
@code{mtxdist}. This makes it easier to avoid possible name clashes
with other code when using libmtx, and also to distinguish parts of a
program that rely on distributed computing from those parts that are
not distributed.

@menu
* Error handling: Error handling for distributed Matrix Market files.  How to handle errors when using libmtx for distributed computing.
* Data structures: Data structures for distributed Matrix Market files. Basic data structures for representing distributed @file{mtx} files.
* Creating distributed Matrix Market files:: How to create distributed @file{mtx} files.
* Converting to and from distributed Matrix Market files:: Functions for converting to and from distributed @file{mtx} files
* Reading and writing distributed Matrix Market files:: Functions for reading from and writing to files in Matrix Market format.
* Transposing, sorting and reordering: Other operations on distributed Matrix Market files. Functions for transposing, sorting and reordering distributed @file{mtx} files.
@end menu


@node Error handling for distributed Matrix Market files
@section Error handling
@cindex error handling
In addition to the error handling routines described in @ref{Error
handling}, libmtx provides some additional error handling
functionality when working with MPI and distributed data. First, some
MPI functions may return an error code on failure, which should be
handled correctly. Second, whenever multiple processes are involved,
there are cases where only one or a few of those processes may
encounter errors. These errors must be handled appropriately to ensure
accurate reporting and that the program exits in a graceful manner
instead of hanging indefinitely.

@subsection MPI errors
@findex mtxdiststrerror
Some functions in libmtx may fail due to MPI errors. In these cases,
some additional information is needed to provide helpful error
descriptions, and the function @code{mtxdiststrerror} should be used
(instead of @code{mtxstrerror}).
@example
@code{const char * mtxdiststrerror(
    int err, int mpierrcode, char * mpierrstr);}
@end example
The error code @code{err} is an integer corresponding to one of the
error codes from the @code{mtxerror} enum type. The arguments
@code{mpierrcode} and @code{mpierrstr} are only used if @code{err} is
@samp{MTX_ERR_MPI}.

@findex MPI_Error_string
@cindex @code{MPI_MAX_ERROR_STRING}
If @code{err} is @samp{MTX_ERR_MPI}, then the argument
@code{mpierrcode} should be set to the error code that was returned
from the MPI function call that failed. In addition, the argument
@code{mpierrstr} must be a char array whose length is at least equal
to @code{MPI_MAX_ERROR_STRING}. Internally, @code{mtxdiststrerror}
uses @code{MPI_Error_string} to obtain a description of the error.

The example below shows how @code{mtxdiststrerror} is typically used.
@example
@code{int err, mpierr;
char mpierrstr[MPI_MAX_ERROR_STRING];
struct mtxdisterror disterr;
err = mtxdisterror_alloc(&disterr, MPI_COMM_WORLD, &mpierr);
if (err) @{
    fprintf(stderr, "error: %s\n",
            mtxdiststrerror(err, mpierr, mpierrstr));
    MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);
@}}
@end example
If @code{mtxdisterror_alloc} returns @samp{MTX_ERR_MPI} and
@code{mpierr} is set to @samp{MPI_ERR_COMM}, then the following
message will be printed:
@example
@code{error: MPI_ERR_COMM: invalid communicator}
@end example


@subsection Distributed error handling
@tindex struct mtxdisterror
To more easily handle errors in cases where one or more processes may
fail, libmtx uses the data type @code{struct mtxdisterror}. Most of
the functions in libmtx that involve distributed computing take an
additional argument of type @code{struct mtxdisterror} to provide
robust error handling in these cases.

@findex mtxdisterror_alloc
To use @code{struct mtxdisterror}, one must first allocate storage
using @code{mtxdisterror_alloc}.
@example
@code{int mtxdisterror_alloc(
    struct mtxdisterror * disterr,
    MPI_Comm comm,
    int * mpierrcode);}
@end example
An example of this was already shown in the previous section.

@findex mtxdisterror_free
Note that the storage allocated for @code{mtxdisterror} should be
freed by calling @code{mtxdisterror_free}.
@example
@code{void mtxdisterror_free(struct mtxdisterror * disterr);}
@end example

@findex mtxdisterror_description
If an error occurs, then a description of the error can be obtained by
calling @code{mtxdisterror_description}.
@example
@code{char * mtxdisterror_description(struct mtxdisterror * disterr);}
@end example
Note that if @code{mtxdisterror_description} is called more than once,
the pointer that was returned from the previous call will no longer be
valid and using it will result in a use-after-free error.

@findex mtxdisterror_allreduce
Finally, the function @code{mtxdisterror_allreduce} can be used to
communicate error status among multiple processes.
@example
@code{int mtxdisterror_allreduce(struct mtxdisterror * disterr, int err);}
@end example
More specifically, @code{mtxdisterror_allreduce} performs a collective
reduction on error codes provided by each MPI process in the
communicator used by @code{disterr}. This is the same MPI communicator
that was provided as the @code{comm} argument to
@code{mtxdisterror_alloc}.

Because @code{mtxdisterror_allreduce} is a collective operation, it
must be performed by every process in the communicator of
@code{disterr}. Otherwise, the program may hang indefinitely.

Each process gathers the error code and rank of every other process.
If the error code of each and every process is @samp{MTX_SUCCESS},
then @code{mtxdisterror_allreduce} returns
@samp{MTX_SUCCESS}. Otherwise, @samp{MTX_ERR_MPI_COLLECTIVE} is
returned.  Moreover, the rank and error code of each process is stored
in @code{disterr}.

If the error code @code{err} is @samp{MTX_ERR_MPI_COLLECTIVE}, then it
is assumed that a reduction has already been performed, and
@code{mtxdisterror_allreduce} returns immediately with
@samp{MTX_ERR_MPI_COLLETIVE}. As a result, if any process calls
@code{mtxdisterror_allreduce} with @code{err} set to
@samp{MTX_ERR_MPI_COLLETIVE}, then every other process in the
communicator must also set @code{err} to
@samp{MTX_ERR_MPI_COLLECTIVE}, or else the program may hang
indefinitely.

The following example shows how @code{mtxdisterror_allreduce} is used.
@example
@code{int err;
struct mtxdisterror disterr;
err = mtxdisterror_alloc(&disterr, MPI_COMM_WORLD);
if (err)
    MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);

// Get the MPI rank of the current process.
// Perform an all-reduction on the error code from
// MPI_Comm_rank, so that if any process fails,
// then we can exit gracefully.
int comm_err, rank;
err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);
comm_err = mtxdisterror_allreduce(&disterr, err);
if (comm_err)
    return comm_err;

...}
@end example


@node Data structures for distributed Matrix Market files
@section Data structures
@tindex struct mtxdistfile
The file @file{libmtx/mtxdistfile/mtxdistfile.h} defines the type
@code{struct mtxdistfile}, which is used to represent a Matrix Market
file distributed among one or more MPI processes. The definition of
the @code{mtxdistfile} struct is shown below.
@example
@code{struct mtxdistfile @{
    MPI_Comm comm;
    int comm_size;
    int rank;
    struct mtxfileheader header;
    struct mtxfilecomments comments;
    struct mtxfilesize size;
    enum mtxprecision precision;
    struct mtxfile mtxfile;
@};}
@end example
The first three struct members contain some information about the
group of processes sharing the distributed Matrix Market file,
including their MPI communicator (@code{comm}), the number of
processes (@code{comm_size}) and the rank of the current process
(@code{rank}). Thereafter, follow the header line, comments, size line
and the chosen precision, all of which must be identical on every
process in the communicator. Finally, @code{mtxfile} represents the
part of the Matrix Market file that resides on the current process.

@node Creating distributed Matrix Market files
@section Creating distributed Matrix Market files
Constructing distributed Matrix Market files works in much the same
way as the non-distributed case, which was described in @ref{Creating
Matrix Market files}.

@findex mtxdistfile_init
If the needed data has already been distributed, then
@code{mtxdistfile_init} can be used to create a distributed Matrix
Market file from Matrix Market files residing on each process in a
communicator.
@example
@code{int mtxdistfile_init(
    struct mtxdistfile * mtxdistfile,
    const struct mtxfile * mtxfile,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
This function performs collective communication and therefore requires
every process in the communicator to perform matching calls to the
function. The communicator must be the same one that was used to
allocate @code{disterr}.

Furthermore, the Matrix Market files on each process must have the
same header line (i.e., object, format, field and symmetry) and
precision. In addition, the number of columns must be the same for
every process in the case of a matrix or vector in array format,
whereas both the number of rows and columns must be the same for every
process in the case of a matrix or vector in coordinate format.

@findex mtxdistfile_free
The function @code{mtxdistfile_free} frees storage allocated for a
distributed Matrix Market file.
@example
@code{void mtxdistfile_free(struct mtxdistfile * mtxdistfile);}
@end example

@findex mtxdistfile_alloc_copy
@findex mtxdistfile_init_copy
To allocate storage for a copy of an existing @code{mtxdistfile}, the
function @code{mtxdistfile_alloc_copy} is used. This function does not
initialise the underlying matrix or vector values. If the matrix or
vector values should also be copied, then @code{mtxdistfile_init_copy}
is used.
@example
@code{int mtxdistfile_alloc_copy(
    struct mtxdistfile * dst,
    const struct mtxdistfile * src,
    struct mtxdisterror * disterr);

int mtxdistfile_init_copy(
    struct mtxdistfile * dst,
    const struct mtxdistfile * src,
    struct mtxdisterror * disterr);}
@end example

@node Creating distributed Matrix Market files in array format
@subsection Creating distributed @file{mtx} files in array format
@findex mtxdistfile_alloc_matrix_array
@findex mtxdistfile_alloc_vector_array
The functions @code{mtxdistfile_alloc_matrix_array} and
@code{mtxdistfile_alloc_vector_array} are used to allocate distributed
matrices and vectors in array format.
@example
@code{int mtxdistfile_alloc_matrix_array(
    struct mtxdistfile * mtxdistfile,
    enum mtxfilefield field,
    enum mtxfilesymmetry symmetry,
    enum mtxprecision precision,
    int num_rows, int num_columns,
    MPI_Comm comm,
    struct mtxdisterror * disterr);

int mtxdistfile_alloc_vector_array(
    struct mtxdistfile * mtxdistfile,
    enum mtxfilefield field,
    enum mtxprecision precision,
    int num_rows,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
@code{field} must be @samp{mtxfile_real}, @samp{mtxfile_complex} or
@samp{mtxfile_integer}. Moreover, @code{field} and @code{precision}
must be the same on every process in the MPI communicator. Because
matrices in array format are distributed according to a
one-dimensional row distribution, @code{num_columns} must also be the
same on every process. However, @code{num_rows} is allowed to differ
between processes and should be used to specify the number of rows of
the matrix or vector that will reside on the current process. The
total number of rows in the distributed matrix is obtained by adding
together the number of rows on each process.

The above functions allocate storage, but they do not initialise the
underlying matrix or vector values. It is therefore up to the user to
initialise these values.

@findex mtxdistfile_init_matrix_array_real_single
@findex mtxdistfile_init_matrix_array_real_double
@findex mtxdistfile_init_matrix_array_complex_single
@findex mtxdistfile_init_matrix_array_complex_double
@findex mtxdistfile_init_matrix_array_integer_single
@findex mtxdistfile_init_matrix_array_integer_double
@findex mtxdistfile_init_matrix_array_pattern
@findex mtxdistfile_init_vector_array_real_single
@findex mtxdistfile_init_vector_array_real_double
@findex mtxdistfile_init_vector_array_complex_single
@findex mtxdistfile_init_vector_array_complex_double
@findex mtxdistfile_init_vector_array_integer_single
@findex mtxdistfile_init_vector_array_integer_double
@findex mtxdistfile_init_vector_array_pattern
If the matrix or vector values are already known, then the functions
@code{mtxdistfile_init_@var{object}_array_@var{field}_@var{precision}}
can be used to allocate storage and initialise values. Here
@code{@var{object}}, @code{@var{field}} and @code{@var{precision}}
correspond to the desired object (@samp{matrix} or @samp{vector}),
field (@samp{real}, @samp{complex} or @samp{integer}), and precision
(@samp{single} or @samp{double}). For example, for a distributed
matrix in array format with real, single precision coefficients, the
function @code{mtxdistfile_init_matrix_array_real_single} is used, as
shown below.
@example
@code{int mtxdistfile_init_matrix_array_real_single(
    struct mtxdistfile * mtxdistfile,
    enum mtxfilesymmetry symmetry,
    int num_rows, int num_columns,
    const float * data,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
The corresponding function for a vector is
@code{mtxdistfile_init_vector_array_real_single}.
@example
@code{int mtxdistfile_init_vector_array_real_single(
    struct mtxdistfile * mtxdistfile,
    int num_rows,
    const float * data,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example

@node Creating distributed Matrix Market files in coordinate format
@subsection Creating distributed @file{mtx} files in coordinate format
@findex mtxdistfile_alloc_matrix_coordinate
@findex mtxdistfile_alloc_vector_coordinate
Matrices and vectors in coordinate format are created in a similar way
to what was shown in the previous section. The functions
@code{mtxdistfile_alloc_matrix_coordinate} and
@code{mtxdistfile_alloc_vector_coordinate} can be used to allocate
distributed matrices and vectors in coordinate format.
@example
@code{int mtxdistfile_alloc_matrix_coordinate(
    struct mtxdistfile * mtxdistfile,
    enum mtxfilefield field,
    enum mtxfilesymmetry symmetry,
    enum mtxprecision precision,
    int num_rows, int num_columns, int64_t num_nonzeros,
    MPI_Comm comm,
    struct mtxdisterror * disterr);

int mtxdistfile_alloc_vector_coordinate(
    struct mtxdistfile * mtxdistfile,
    enum mtxfilefield field,
    enum mtxprecision precision,
    int num_rows, int64_t num_nonzeros,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
The main differences compared to array formats are: @code{field} is
allowed to be @samp{mtxfile_pattern}, an additional argument
(@code{num_nonzeros}) is needed to specify the number of (nonzero)
matrix or vector entries, and @code{num_rows} and @code{num_columns}
must have the same values on every process. However,
@code{num_nonzeros} may have different values on different processes,
and each process should use this argument to specify the number of
matrix or vector entries that will reside on the current process. The
total number of matrix or vector entries in the distributed matrix is
obtained by adding together the number of entries on each process.

@findex mtxdistfile_init_matrix_coordinate_real_single
@findex mtxdistfile_init_matrix_coordinate_real_double
@findex mtxdistfile_init_matrix_coordinate_complex_single
@findex mtxdistfile_init_matrix_coordinate_complex_double
@findex mtxdistfile_init_matrix_coordinate_integer_single
@findex mtxdistfile_init_matrix_coordinate_integer_double
@findex mtxdistfile_init_matrix_coordinate_pattern
@findex mtxdistfile_init_vector_coordinate_real_single
@findex mtxdistfile_init_vector_coordinate_real_double
@findex mtxdistfile_init_vector_coordinate_complex_single
@findex mtxdistfile_init_vector_coordinate_complex_double
@findex mtxdistfile_init_vector_coordinate_integer_single
@findex mtxdistfile_init_vector_coordinate_integer_double
@findex mtxdistfile_init_vector_coordinate_pattern
The above functions allocate storage, but they do not initialise the
underlying matrix or vector values. It is therefore up to the user to
initialise these values. Alternatively, if the matrix or vector values
are readily available, then the functions
@code{mtxdistfile_init_@var{object}_coordinate_@var{field}_@var{precision}}
can be used to allocate storage and initialise the matrix or vector
values at the same time. As before, @code{@var{object}},
@code{@var{field}} and @code{@var{precision}} correspond to the
desired object (@samp{matrix} or @samp{vector}), field (@samp{real},
@samp{complex}, @samp{integer} or @samp{pattern}), and precision
(@samp{single} or @samp{double}). For example, for a distributed
matrix in coordinate format with real, single precision coefficients,
the function @code{mtxdistfile_init_matrix_coordinate_real_single} is
used, as shown below.
@example
@code{int mtxdistfile_init_matrix_coordinate_real_single(
    struct mtxdistfile * mtxdistfile,
    enum mtxfilesymmetry symmetry,
    int num_rows, int num_columns, int64_t num_nonzeros,
    const struct mtxfile_matrix_coordinate_real_single * data,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
In the case of a vector, the corresponding function is
@code{mtxdistfile_init_vector_coordinate_real_single}.
@example
@code{int mtxdistfile_init_vector_coordinate_real_single(
    struct mtxdistfile * mtxdistfile,
    int num_rows, int64_t num_nonzeros,
    const struct mtxfile_vector_coordinate_real_single * data,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example


@node Setting matrix or vector values for distributed Matrix Market files
@subsection Setting matrix or vector values
@findex mtxdistfile_set_constant_real_single
@findex mtxdistfile_set_constant_real_double
@findex mtxdistfile_set_constant_complex_single
@findex mtxdistfile_set_constant_complex_double
@findex mtxdistfile_set_constant_integer_single
@findex mtxdistfile_set_constant_integer_double
For convenience, the functions
@code{mtxdistfile_set_constant_@var{field}_@var{precision}} are
provided to initialise every value of a distributed matrix or vector
to the same constant. Here @code{@var{field}} and
@code{@var{precision}} should match the field (@samp{real},
@samp{complex}, @samp{integer} or @samp{patter}) and precision
(@samp{single} or @samp{double}) of @code{mtxdistfile}.
@example
@code{int mtxdistfile_set_constant_real_single(
    struct mtxdistfile * mtxdistfile, float a,
    struct mtxdisterror * disterr);

int mtxdistfile_set_constant_real_double(
    struct mtxdistfile * mtxdistfile, double a,
    struct mtxdisterror * disterr);

int mtxdistfile_set_constant_complex_single(
    struct mtxdistfile * mtxdistfile, float a[2],
    struct mtxdisterror * disterr);

int mtxdistfile_set_constant_complex_double(
    struct mtxdistfile * mtxdistfile, double a[2],
    struct mtxdisterror * disterr);

int mtxdistfile_set_constant_integer_single(
    struct mtxdistfile * mtxdistfile, int32_t a,
    struct mtxdisterror * disterr);

int mtxdistfile_set_constant_integer_double(
    struct mtxdistfile * mtxdistfile, int64_t a,
    struct mtxdisterror * disterr);}
@end example

@node Converting to and from distributed Matrix Market files
@section Converting to and from distributed Matrix Market files
This section describes how to convert a Matrix Market file that
resides on a single process to a Matrix Market file that is
distributed among multiple processes.

@findex mtxdistfile_from_mtxfile
The function @code{mtxdistfile_from_mtxfile} takes a Matrix Market
file stored on a single root process and partitions and distributes
the underlying matrix or vector among processes in a communicator.
@example
@code{int mtxdistfile_from_mtxfile(
    struct mtxdistfile * dst,
    const struct mtxfile * src,
    MPI_Comm comm, int root,
    struct mtxdisterror * disterr);}
@end example
This function performs collective communication and therefore requires
every process in the communicator to perform matching calls to the
function.

The Matrix Market file @code{src} is distributed by first broadcasting
the header line and precision from the root process to the other
processes. Next, the number of matrix or vector elements to send to
each process is determined. For matrices in array format, the rows are
evenly distributed among the processes. In all other cases, the total
number of data lines is evenly distributed among the processes.


@node Reading and writing distributed Matrix Market files
@section Reading and writing distributed Matrix Market files
This section explains how to read from and write to files in the
Matrix Market format whenever data is distributed among multiple MPI
processes.

@cindex collective I/O
@cindex distributed I/O
@cindex parallel I/O
@cindex file-per-process
@cindex shared file
In the case of reading or writing a distributed matrix or vector in
Matrix Market format, there are essentially two options. The first
option is the @dfn{file-per-process} model, where each process reads
or writes its part of the matrix or vector to or from its own
file. The second option is the @dfn{shared file} model, where
processes send or receive their data to or from a single root process,
and the root process reads or writes everything from or to a single
file.

Each of the I/O models mentioned above have advantages and
disadvantages. The file-per-process model allows processes to write
their data in parallel, and may therefore be much faster. However,
when a large number of MPI processes are involved, there will also be
a large number of files. This may not only be more difficult to manage
than a single file, but also results in significant overhead due to
the file system's handling of metadata associated with each file. The
shared file model, on the other hand, produces only a single file.
This can be much simpler to deal with and there is no overhead
associated with metadata beyond that one file. Unfortunately, the I/O
performance can be severely limiting due to the fact that only a
single process is responsible for reading from or writing to the file.

@cindex MPI-IO
@cindex HDF5
Note that when using a very large number of MPI processes and very
large files, a high-performance I/O library such as MPI-IO
(@ref{W. Gropp@comma{} E. Lusk and R. Thakur (1999)}) or HDF5
(@ref{HDF5}) may be a better alternative. However, this is not
currently supported in libmtx.

@node Reading distributed Matrix Market files
@subsection Reading distributed Matrix Market files
@findex mtxdistfile_fread
To read an @file{mtx} file from a @code{FILE} stream, partition the
data and distribute it among MPI processes in a communicator based on
the shared file model, use @code{mtxdistfile_fread}:
@example
@code{int mtxdistfile_fread(
    struct mtxdistfile * mtxdistfile,
    enum mtxprecision precision,
    FILE * f,
    int * lines_read, int64_t * bytes_read,
    size_t line_max, char * linebuf,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
For the most part, @code{mtxdistfile_fread} works just like
@code{mtxfile_fread} (see @ref{Reading and writing Matrix Market
files}). If successful, @samp{MTX_SUCCESS} is returned, and
@code{mtxdistfile} will contain the distributed Matrix Market
file. The user is responsible for calling @code{mtxdistfile_free} to
free any storage allocated by @code{mtxdistfile_fread}. If
@code{mtxdistfile_fread} fails, an error code is returned and
@code{lines_read} and @code{bytes_read} are used to indicate the line
number and byte of the Matrix Market file where an error was
encountered. @code{lines_read} and @code{bytes_read} are ignored if
they are set to @samp{NULL}.

@cindex sysconf
@cindex _SC_LINE_MAX
Moreover, @code{precision} is used to choose the precision for storing
the values of matrix or vector entries, as described in
@ref{Precision}. If @code{linebuf} is not @samp{NULL}, then it must
point to an array that can hold a null-terminated string whose length
(including the terminating null-character) is at most @code{line_max}.
This buffer is used for reading lines from the stream. Otherwise, if
@code{linebuf} is @samp{NULL}, then a temporary buffer is allocated
and used, and the maximum line length is determined by calling
@code{sysconf()} with @code{_SC_LINE_MAX}.

Only a single root process will read from the specified stream. The
data is partitioned into equal-sized parts for each process. For
matrices and vectors in coordinate format, the total number of data
lines is evenly distributed among processes. Otherwise, the rows are
evenly distributed among processes.

The file is read one part at a time, which is then sent to the owning
process. This avoids reading the entire file into the memory of the
root process at once, which would severely limit the size of files
that could be read.

This function performs collective communication and therefore requires
every process in the communicator to perform matching calls to the
function.


@cindex zlib
@cindex gzip compression
@findex mtxdistfile_gzread
If libmtx is built with zlib support, then @code{mtxdistfile_gzread}
can be used to read gzip-compressed @file{mtx} files. The data is
partitioned and distributed among MPI processes in the same way as
with @code{mtxdistfile_fread}.
@example
@code{int mtxdistfile_gzread(
    struct mtxdistfile * mtxdistfile,
    enum mtxprecision precision,
    gzFile f,
    int * lines_read, int64_t * bytes_read,
    size_t line_max, char * linebuf,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example

@findex mtxdistfile_read
For convenience, the function @code{mtxdistfile_read} can be used to
read an @file{mtx} file from a given path.
@example
@code{int mtxdistfile_read(
    struct mtxdistfile * mtxdistfile,
    enum mtxprecision precision,
    const char * path,
    bool gzip,
    int * lines_read, int64_t * bytes_read,
    MPI_Comm comm,
    struct mtxdisterror * disterr);}
@end example
The file is assumed to be gzip-compressed if @code{gzip} is
@samp{true}, and uncompressed otherwise. If @code{path} is @samp{-},
then the standard input stream is used.

@node Writing distributed Matrix Market files
@subsection Writing distributed Matrix Market files
@findex mtxdistfile_fwrite
To write a distributed @file{mtx} file to a @code{FILE} stream using
the file-per-process model, use @code{mtxdistfile_fwrite}:
@example
@code{int mtxdistfile_fwrite(
    const struct mtxdistfile * mtxdistfile,
    FILE * f,
    const char * fmt,
    int64_t * bytes_written,
    bool sequential,
    struct mtxdisterror * disterr);}
@end example
Here, @code{f} should point to a different stream on every process.
The processes involved are those from the MPI communicator
@code{mtxdistfile->comm}. If successful, @samp{MTX_SUCCESS} is
returned, and each process wrote its part of the matrix or vector to
its stream. Moreover, if it is not @samp{NULL}, then the number of
bytes written to the stream by the current process is returned in
@code{bytes_written}.

The @code{fmt} argument may optionally be used to specify a format
string for outputting of numerical values, in the same way as with
@code{mtxfile_write} (see @ref{Writing Matrix Market files}).

If @code{sequential} is @samp{true}, then output is performed in
sequence by MPI processes in the communicator. (A barrier is used
between outputting each part of the Matrix Market file.)  This can
sometimes be useful for debugging, to make it more likely that
processes write their data in order without interfering with each
other. However, there is no guarantee that the output will appear in
the correct order when writing to a shared stream (e.g., standard
output). Therefore, if a guaranteed correct order of output is needed,
@code{mtxdistfile_fwrite_shared} should be used instead.

@findex mtxdistfile_fwrite_shared
The function @code{mtxdistfile_fwrite_shared} uses a shared file model
to write a distributed matrix or vector to a single stream shared by
every process in a communicator. This method is based on a shared file
model.
@example
@code{int mtxdistfile_fwrite_shared(
    const struct mtxdistfile * mtxdistfile,
    FILE * f,
    const char * fmt,
    int64_t * bytes_written,
    int root,
    struct mtxdisterror * disterr);}
@end example
If @code{bytes_written} is not @samp{NULL}, then it is used to return
the total number of bytes written to the stream for all parts of the
distributed matrix or vector.

Note that only the specified @code{root} process will write anything
to the stream. Other processes therefore send their part of the
distributed Matrix Market file to the root process for writing.

This function performs collective communication and therefore
requires every process in the communicator to perform matching
calls to the function.


@findex mtxdistfile_gzwrite
@findex mtxdistfile_gzwrite_shared
If libmtx is built with zlib support, then @code{mtxdistfile_gzwrite}
and @code{mtxdistfile_gzwrite_shared} can be used to write
gzip-compressed @file{mtx} files using the file-per-process and shared
file models, respectively.
@example
@code{int mtxdistfile_fwrite(
    const struct mtxdistfile * mtxdistfile,
    gzFile f,
    const char * fmt,
    int64_t * bytes_written,
    bool sequential,
    struct mtxdisterror * disterr);

int mtxdistfile_fwrite_shared(
    const struct mtxdistfile * mtxdistfile,
    gzFile f,
    const char * fmt,
    int64_t * bytes_written,
    int root,
    struct mtxdisterror * disterr);}
@end example

@findex mtxdistfile_write
@findex mtxdistfile_write_shared
For convenience, the functions @code{mtxdistfile_write} and
@code{mtxdistfile_write_shared} can be used to write a distributed
@file{mtx} file to given paths.
@example
@code{int mtxdistfile_write(
    const struct mtxdistfile * mtxdistfile,
    const char * path,
    bool gzip,
    const char * fmt,
    int64_t * bytes_written,
    bool sequential,
    struct mtxdisterror * disterr);

int mtxdistfile_write_shared(
    const struct mtxdistfile * mtxdistfile,
    const char * path,
    bool gzip,
    const char * fmt,
    int64_t * bytes_written,
    int root,
    struct mtxdisterror * disterr);}
@end example
The files are written as gzip-compressed streams if @code{gzip} is
@samp{true}, and uncompressed otherwise. If @code{path} is @samp{-},
then the standard output stream is used.


@node Other operations on distributed Matrix Market files
@section Transposing, sorting and reordering



@c This section describes the basic data types used to represent
@c distributed matrices and vectors.

@c @tindex struct mtxdist
@c @tindex mtxdist
@c The file @file{libmtx/mtxdist.h} defines the @code{struct
@c mtxdist} type, which is used to represent distributed objects in the
@c Matrix Market format. The definition of the @code{mtxdist} struct is shown
@c below.
@c @example
@c @code{struct mtxdist @{
@c   /* Data distribution */
@c   MPI_Comm comm;
@c   enum mtx_distribution row_distribution;
@c   enum mtx_distribution column_distribution;
@c   int64_t num_global_rows;
@c   int64_t num_global_columns;
@c   int num_block_rows;
@c   int num_block_columns;
@c   int block_row_size;
@c   int block_column_size;
@c   int block_row;
@c   int block_column;
@c   int64_t * global_rows;
@c   int64_t * global_columns;

@c   /* Matrix Market object */
@c   struct mtx * mtx;
@c @};}
@c @end example

@c The @code{mtxdist} struct contains information about how the
@c underlying matrix or vector is distributed among processes.  It also
@c contains a member of type @code{struct mtx}, which, on a given MPI
@c process, represents the underlying, local matrix of the current
@c process.


@c The following sections provide a detailed explanation of the
@c @code{mtxdist} struct members and their data types.


@c @node Data distribution
@c @subsection Data distribution

@c @cindex data distribution
@c @cindex distributed matrix
@c @cindex distributed vector
@c @cindex block distribution
@c @cindex cyclic distribution
@c @cindex block-cyclic distribution
@c @cindex discrete distribution
@c @tindex mtx_distribution
@c It is often necessary to distribute large matrices and vectors across
@c multiple processes, both for the purpose of performing computations in
@c parallel and also to use multiple nodes, thereby increasing the total
@c amount of available memory.  To facilitate such data distribution,
@c some additional information is stored in the @code{mtx} struct.

@c First, we define the additional enum type @code{mtx_distribution},
@c which describes different methods for distributing a one-dimensional
@c data structure, such as a vector, among multiple processes.  Matrices
@c are distributed by independently specifying the distributions of the
@c rows and columns.
@c @example
@c @code{enum mtx_distribution @{
@c     mtx_private,           /* owned by a single process */
@c     mtx_replicated,        /* replicated across every process */
@c     mtx_block,             /* block distribution */
@c     mtx_cyclic,            /* cyclic distribution */
@c     mtx_block_cyclic,      /* block-cyclic distribution */
@c     mtx_discrete,          /* discrete distribution */
@c @};}
@c @end example
@c By default, matrices and vectors are not distributed
@c (@code{mtx_private}).  That is, the entries of a vector and the rows
@c and columns of a matrix are owned by a single process.

@c For a distributed vector, @code{mtx_block} is used when the vector is
@c partitioned into contiguous blocks of roughly equal size and one block
@c is assigned to each process.  In contrast, @code{mtx_cyclic} assigns
@c consecutive entries of the vector to successive processes.  By
@c generalising the block and cyclic distributions,
@c @code{mtx_block_cyclic} assigns consecutive, fixed-size blocks to
@c successive processes.  Finally, @code{mtx_discrete} allows an
@c arbitrary assignment of global vector entries to processes.


@c @cindex cover
@c @cindex partition
@c @tindex mtx_partitioning
@c The enum type @code{mtx_partitioning}, is used to describe whether the
@c rows and columns of a distributed matrix or vector form a partition or
@c merely a cover of the rows and columns of a global matrix or
@c vector. In the case of a partition, each matrix or vector entry is
@c owned by a single MPI process. In the case of a cover, different MPI
@c processes are allowed to store values associated with the same matrix
@c or vector entry.
@c @example
@c @code{enum mtx_partitioning @{
@c     mtx_partition,   /* matrix/vector entries are owned
@c                          * by a single MPI process. */
@c     mtx_cover,       /* matrix/vector entries may be owned
@c                          * by multiple MPI processes. */
@c @};}
@c @end example
@c Note that some algorithms may only work with a partitioned matrix and
@c might produce incorrect results in the case of a covering. Thus, it
@c may be necessary to first perform a reduction to combine values
@c associated with matrix or vector entries that are distributed across
@c multiple MPI processes.


@c @node Index sets
@c @subsection Index sets

@c @cindex Index set
@c An @dfn{index set} is a set of integers, typically used to represent a
@c subset of the rows of a vector or the rows or columns of a
@c matrix. Index sets are used, for example, when specifying submatrices
@c of a matrix, or for partitioning and distributing matrices and vectors
@c among multiple processes.

@c @tindex struct mtx_index_set
@c @tindex enum mtx_index_set_type
@c The file @file{libmtx/util/index_set.h} defines data types for index
@c sets, including @code{struct mtx_index_set}. There are different types
@c of index sets, which may be distinguished by the enum type
@c @code{mtx_index_set_type}.
@c @itemize
@c @item @code{mtx_index_set_interval}
@c represents an index set of contiguous integers from a half-open
@c interval @code{[a,b)}.

@c @item @code{mtx_index_set_array}
@c represents a discrete index set, which is not necessarily contiguous,
@c as an array of integers.

@c @end itemize

@c An index set representing a half-open interval @code{[a,b)} can be
@c created with @code{mtx_index_set_init_interval}.
@c @findex mtx_index_set_init_interval
@c @example
@c @code{int mtx_index_set_init_interval(
@c     struct mtx_index_set * index_set, int a, int b);}
@c @end example
@c Then, the function @code{mtx_index_set_contains} can be used to test if
@c a given integer @code{n} belongs to the index set.
@c @findex mtx_index_set_contains
@c @example
@c @code{bool mtx_index_set_contains(
@c     const struct mtx_index_set * index_set, int n);}
@c @end example


@c @node Creating distributed matrices and vectors
@c @section Creating distributed matrices and vectors
@c A number of functions are provided to more conveniently construct
@c distributed matrices and vectors. These are described in the following
@c subsections.


@c @node mtxdist_free
@c @subsection mtxdist_free

@c @findex mtxdist_free
@c Since a distributed matrix or vector represented by a @code{struct
@c mtxdist} allocates some storage for its data, the user is required to
@c free the allocated storage by calling @code{mtxdist_free} when they
@c are finished with the matrix or vector:
@c @example
@c @code{void mtxdist_free(
@c     struct mtxdist * mtxdist);}
@c @end example


@c @node Creating distributed vectors
@c @subsection Creating distributed vectors


@c @node Creating distributed matrices
@c @subsection Creating distributed matrices


@c @node Reading and writing distributed Matrix Market files
@c @section Reading and writing distributed Matrix Market files


@c @node Communicating matrices and vectors
@c @section Communicating matrices and vectors

@c The file @file{libmtx/mtx/mpi.h} defines functions that can be
@c used to communicate Matrix Market objects represented by the
@c @code{mtx} struct between MPI processes.


@c @node MPI errors
@c @subsection MPI errors

@c @cindex MPI errors
@c @findex mtxdiststrerror
@c In the event of an MPI-related error, then the above functions return
@c @code{MTX_ERR_MPI} and the argument @code{mpierrcode} is set to a
@c specific MPI error code. @code{mpierrcode} can then be used with the
@c function @code{mtxdiststrerror}, as described in @ref{Error handling}.


@c @node send receive broadcast
@c @subsection Send, receive and broadcast

@c The basic functions for communicating @code{struct mtx} objects are:
@c @example
@c @code{int mtx_send(
@c     const struct mtx * mtx,
@c     int dest,
@c     int tag,
@c     MPI_Comm comm,
@c     int * mpierrcode);

@c int mtx_recv(
@c     struct mtx * mtx,
@c     int source,
@c     int tag,
@c     MPI_Comm comm,
@c     int * mpierrcode);

@c int mtx_bcast(
@c     struct mtx * mtx,
@c     int root,
@c     MPI_Comm comm,
@c     int * mpierrcode);}
@c @end example
@c These functions are analogous to @code{MPI_Send}, @code{MPI_Recv} and
@c @code{MPI_Bcast}.
