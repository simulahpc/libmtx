@c This file is part of libmtx.
@c Copyright (C) 2022 James D. Trotter
@c
@c libmtx is free software: you can redistribute it and/or modify it
@c under the terms of the GNU General Public License as published by
@c the Free Software Foundation, either version 3 of the License, or
@c (at your option) any later version.
@c
@c libmtx is distributed in the hope that it will be useful, but
@c WITHOUT ANY WARRANTY; without even the implied warranty of
@c MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
@c General Public License for more details.
@c
@c You should have received a copy of the GNU General Public License
@c along with libmtx.  If not, see <https://www.gnu.org/licenses/>.
@c
@c Authors: James D. Trotter <james@simula.no>
@c Last modified: 2022-01-06
@c
@c libmtx User Guide: Distributed Matrix Market files.

@node Distributed Matrix Market files
@chapter Distributed Matrix Market files

This chapter describes how to distribute Matrix Market files among
multiple processes using MPI, and how to perform various operations on
those files in a distributed manner. The distributed-memory computing
features in libmtx are implemented using MPI, and you will therefore
need to build libmtx with MPI support.

For most user-facing types and functions that relate to distributed
computing, libmtx uses the convention of prefixing their names with
@code{mtxdist}. This makes it easier to avoid possible name clashes
with other code when using libmtx, and also to distinguish parts of a
program that rely on distributed computing from those parts that are
not distributed.

@menu
* Error handling: MPI error handling.  How to handle errors when using libmtx for distributed computing.
@c * Data types:: Basic data types for representing distributed matrices and vectors.
@c * MPI error handling:: How to handle errors when working with MPI and distributed matrices and vectors.
@c * Creating distributed matrices and vectors:: Functions for creating distributed matrices and vectors.
@c * Reading and writing distributed Matrix Market files:: Functions for reading from and writing to files in Matrix Market format.
@c * Communicating matrices and vectors:: Message-passing functions for Matrix Market objects
@end menu

@node MPI error handling
@section Error handling

@cindex MPI errors
@findex mtxdiststrerror
In addition to the error handling routines described @ref{Error
handling}, libmtx provides some additional error handling
functionality when working with MPI and distributed data. First, some
MPI functions return error codes on failure, which should be handled
correctly. Second, whenever multiple processes are involved, there are
cases where only one or a few of those processes may encounter
errors. These errors must be handled appropriately to ensure accurate
reporting and that the program exits in a graceful manner instead of
hanging.

@subsection MPI errors
Some functions in libmtx may fail due to MPI errors. In these cases,
some additional information is needed to provide helpful error
descriptions, and the function @code{mtxdiststrerror} should be used.
@example
@code{const char * mtxdiststrerror(
    int err, int mpierrcode, char * mpierrstr);}
@end example
The error code @code{err} is an integer corresponding to one of the
error codes from the @code{mtxerror} enum type. The arguments
@code{mpierrcode} and @code{mpierrstr} are only used if @code{err} is
@code{MTX_ERR_MPI}.

@findex MPI_Error_string
@cindex @code{MPI_MAX_ERROR_STRING}
If @code{err} is @code{MTX_ERR_MPI}, then the argument
@code{mpierrcode} should be set to the error code that was returned
from the MPI function call that failed. In addition, the argument
@code{mpierrstr} must be a char array whose length is at least equal
to @code{MPI_MAX_ERROR_STRING}. Internally, @code{mtxdiststrerror}
uses @code{MPI_Error_string} to obtain a description of the error.

The example below shows how @code{mtxdiststrerror} is typically used.
@example
@code{int err, mpierr;
char mpierrstr[MPI_MAX_ERROR_STRING];
struct mtxdisterror disterr;
err = mtxdisterror_alloc(&disterr, MPI_COMM_WORLD, &mpierr);
if (err) @{
    fprintf(stderr, "error: %s\n",
            mtxdiststrerror(err, mpierr, mpierrstr));
    MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);
@}}
@end example
If @code{mtxdisterror_alloc} returns @samp{MTX_ERR_MPI} and
@code{mpierr} is set to @samp{MPI_ERR_COMM}, then the following
message will be printed:
@example
@code{error: MPI_ERR_COMM: invalid communicator}
@end example


@subsection Distributed error handling
To more easily handle errors in cases where one or more processes may
fail, libmtx uses the data type @code{struct mtxdisterror}. Most of
the functions in libmtx that involve distributed computing take an
additional argument of type @code{struct mtxdisterror} to provide
robust error handling in these cases.

To use @code{struct mtxdisterror}, one must first allocate storage
using @code{mtxdisterror_alloc}.
@example
@code{int mtxdisterror_alloc(
    struct mtxdisterror * disterr,
    MPI_Comm comm,
    int * mpierrcode);}
@end example
An example of this was already shown in the previous section.

Note that the storage allocated for @code{mtxdisterror} should be
freed by calling @code{mtxdisterror_free}.
@example
@code{void mtxdisterror_free(struct mtxdisterror * disterr);}
@end example

If an error occurs, then a description of the error can be obtained by
calling @code{mtxdisterror_description}.
@example
@code{char * mtxdisterror_description(struct mtxdisterror * disterr);}
@end example
Note that if @code{mtxdisterror_description} is called more than once,
the pointer that was returned from the previous call will no longer be
valid and using it will result in a use-after-free error.

Finally, the function @code{mtxdisterror_allreduce} can be used to
communicate error status among multiple processes.
@example
@code{int mtxdisterror_allreduce(struct mtxdisterror * disterr, int err);}
@end example
More specifically, @code{mtxdisterror_allreduce} performs a collective
reduction on error codes provided by each MPI process in the
communicator used by @code{disterr}. This is the same MPI communicator
that was provided as the @code{comm} argument to
@code{mtxdisterror_alloc}.

Because @code{mtxdisterror_allreduce} is a collective operation, it
must be performed by every process in the communicator of
@code{disterr}. Otherwise, the program may hang indefinitely.

Each process gathers the error code and rank of every other process.
If the error code of each and every process is @samp{MTX_SUCCESS},
then @code{mtxdisterror_allreduce} returns
@samp{MTX_SUCCESS}. Otherwise, @samp{MTX_ERR_MPI_COLLECTIVE} is
returned.  Moreover, the rank and error code of each process is stored
in @code{disterr}.

If the error code @code{err} is @samp{MTX_ERR_MPI_COLLECTIVE}, then it
is assumed that a reduction has already been performed, and
@code{mtxdisterror_allreduce} returns immediately with
@samp{MTX_ERR_MPI_COLLETIVE}. As a result, if any process calls
@code{mtxdisterror_allreduce} with @code{err} set to
@samp{MTX_ERR_MPI_COLLETIVE}, then every other process in the
communicator must also set @code{err} to
@samp{MTX_ERR_MPI_COLLECTIVE}, or else the program may hang
indefinitely.

The following example shows how @code{mtxdisterror_allreduce} is used.
@example
@code{int err;
struct mtxdisterror disterr;
err = mtxdisterror_alloc(&disterr, MPI_COMM_WORLD);
if (err)
    MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);

// Get the MPI rank of the current process.
// Perform an all-reduction on the error code from
// MPI_Comm_rank, so that if any process fails,
// then we can exit gracefully.
int comm_err, rank;
err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);
comm_err = mtxdisterror_allreduce(&disterr, err);
if (comm_err)
    return comm_err;

...}
@end example


@c @node Data types
@c @section Data types

@c This section describes the basic data types used to represent
@c distributed matrices and vectors.

@c @tindex struct mtxdist
@c @tindex mtxdist
@c The file @file{libmtx/mtxdist.h} defines the @code{struct
@c mtxdist} type, which is used to represent distributed objects in the
@c Matrix Market format. The definition of the @code{mtxdist} struct is shown
@c below.
@c @example
@c @code{struct mtxdist @{
@c   /* Data distribution */
@c   MPI_Comm comm;
@c   enum mtx_distribution row_distribution;
@c   enum mtx_distribution column_distribution;
@c   int64_t num_global_rows;
@c   int64_t num_global_columns;
@c   int num_block_rows;
@c   int num_block_columns;
@c   int block_row_size;
@c   int block_column_size;
@c   int block_row;
@c   int block_column;
@c   int64_t * global_rows;
@c   int64_t * global_columns;

@c   /* Matrix Market object */
@c   struct mtx * mtx;
@c @};}
@c @end example

@c The @code{mtxdist} struct contains information about how the
@c underlying matrix or vector is distributed among processes.  It also
@c contains a member of type @code{struct mtx}, which, on a given MPI
@c process, represents the underlying, local matrix of the current
@c process.


@c The following sections provide a detailed explanation of the
@c @code{mtxdist} struct members and their data types.


@c @node Data distribution
@c @subsection Data distribution

@c @cindex data distribution
@c @cindex distributed matrix
@c @cindex distributed vector
@c @cindex block distribution
@c @cindex cyclic distribution
@c @cindex block-cyclic distribution
@c @cindex discrete distribution
@c @tindex mtx_distribution
@c It is often necessary to distribute large matrices and vectors across
@c multiple processes, both for the purpose of performing computations in
@c parallel and also to use multiple nodes, thereby increasing the total
@c amount of available memory.  To facilitate such data distribution,
@c some additional information is stored in the @code{mtx} struct.

@c First, we define the additional enum type @code{mtx_distribution},
@c which describes different methods for distributing a one-dimensional
@c data structure, such as a vector, among multiple processes.  Matrices
@c are distributed by independently specifying the distributions of the
@c rows and columns.
@c @example
@c @code{enum mtx_distribution @{
@c     mtx_private,           /* owned by a single process */
@c     mtx_replicated,        /* replicated across every process */
@c     mtx_block,             /* block distribution */
@c     mtx_cyclic,            /* cyclic distribution */
@c     mtx_block_cyclic,      /* block-cyclic distribution */
@c     mtx_discrete,          /* discrete distribution */
@c @};}
@c @end example
@c By default, matrices and vectors are not distributed
@c (@code{mtx_private}).  That is, the entries of a vector and the rows
@c and columns of a matrix are owned by a single process.

@c For a distributed vector, @code{mtx_block} is used when the vector is
@c partitioned into contiguous blocks of roughly equal size and one block
@c is assigned to each process.  In contrast, @code{mtx_cyclic} assigns
@c consecutive entries of the vector to successive processes.  By
@c generalising the block and cyclic distributions,
@c @code{mtx_block_cyclic} assigns consecutive, fixed-size blocks to
@c successive processes.  Finally, @code{mtx_discrete} allows an
@c arbitrary assignment of global vector entries to processes.


@c @cindex cover
@c @cindex partition
@c @tindex mtx_partitioning
@c The enum type @code{mtx_partitioning}, is used to describe whether the
@c rows and columns of a distributed matrix or vector form a partition or
@c merely a cover of the rows and columns of a global matrix or
@c vector. In the case of a partition, each matrix or vector entry is
@c owned by a single MPI process. In the case of a cover, different MPI
@c processes are allowed to store values associated with the same matrix
@c or vector entry.
@c @example
@c @code{enum mtx_partitioning @{
@c     mtx_partition,   /* matrix/vector entries are owned
@c                          * by a single MPI process. */
@c     mtx_cover,       /* matrix/vector entries may be owned
@c                          * by multiple MPI processes. */
@c @};}
@c @end example
@c Note that some algorithms may only work with a partitioned matrix and
@c might produce incorrect results in the case of a covering. Thus, it
@c may be necessary to first perform a reduction to combine values
@c associated with matrix or vector entries that are distributed across
@c multiple MPI processes.


@c @node Index sets
@c @subsection Index sets

@c @cindex Index set
@c An @dfn{index set} is a set of integers, typically used to represent a
@c subset of the rows of a vector or the rows or columns of a
@c matrix. Index sets are used, for example, when specifying submatrices
@c of a matrix, or for partitioning and distributing matrices and vectors
@c among multiple processes.

@c @tindex struct mtx_index_set
@c @tindex enum mtx_index_set_type
@c The file @file{libmtx/util/index_set.h} defines data types for index
@c sets, including @code{struct mtx_index_set}. There are different types
@c of index sets, which may be distinguished by the enum type
@c @code{mtx_index_set_type}.
@c @itemize
@c @item @code{mtx_index_set_interval}
@c represents an index set of contiguous integers from a half-open
@c interval @code{[a,b)}.

@c @item @code{mtx_index_set_array}
@c represents a discrete index set, which is not necessarily contiguous,
@c as an array of integers.

@c @end itemize

@c An index set representing a half-open interval @code{[a,b)} can be
@c created with @code{mtx_index_set_init_interval}.
@c @findex mtx_index_set_init_interval
@c @example
@c @code{int mtx_index_set_init_interval(
@c     struct mtx_index_set * index_set, int a, int b);}
@c @end example
@c Then, the function @code{mtx_index_set_contains} can be used to test if
@c a given integer @code{n} belongs to the index set.
@c @findex mtx_index_set_contains
@c @example
@c @code{bool mtx_index_set_contains(
@c     const struct mtx_index_set * index_set, int n);}
@c @end example


@c @node Creating distributed matrices and vectors
@c @section Creating distributed matrices and vectors
@c A number of functions are provided to more conveniently construct
@c distributed matrices and vectors. These are described in the following
@c subsections.


@c @node mtxdist_free
@c @subsection mtxdist_free

@c @findex mtxdist_free
@c Since a distributed matrix or vector represented by a @code{struct
@c mtxdist} allocates some storage for its data, the user is required to
@c free the allocated storage by calling @code{mtxdist_free} when they
@c are finished with the matrix or vector:
@c @example
@c @code{void mtxdist_free(
@c     struct mtxdist * mtxdist);}
@c @end example


@c @node Creating distributed vectors
@c @subsection Creating distributed vectors


@c @node Creating distributed matrices
@c @subsection Creating distributed matrices


@c @node Reading and writing distributed Matrix Market files
@c @section Reading and writing distributed Matrix Market files


@c @node Communicating matrices and vectors
@c @section Communicating matrices and vectors

@c The file @file{libmtx/mtx/mpi.h} defines functions that can be
@c used to communicate Matrix Market objects represented by the
@c @code{mtx} struct between MPI processes.


@c @node MPI errors
@c @subsection MPI errors

@c @cindex MPI errors
@c @findex mtxdiststrerror
@c In the event of an MPI-related error, then the above functions return
@c @code{MTX_ERR_MPI} and the argument @code{mpierrcode} is set to a
@c specific MPI error code. @code{mpierrcode} can then be used with the
@c function @code{mtxdiststrerror}, as described in @ref{Error handling}.


@c @node send receive broadcast
@c @subsection Send, receive and broadcast

@c The basic functions for communicating @code{struct mtx} objects are:
@c @example
@c @code{int mtx_send(
@c     const struct mtx * mtx,
@c     int dest,
@c     int tag,
@c     MPI_Comm comm,
@c     int * mpierrcode);

@c int mtx_recv(
@c     struct mtx * mtx,
@c     int source,
@c     int tag,
@c     MPI_Comm comm,
@c     int * mpierrcode);

@c int mtx_bcast(
@c     struct mtx * mtx,
@c     int root,
@c     MPI_Comm comm,
@c     int * mpierrcode);}
@c @end example
@c These functions are analogous to @code{MPI_Send}, @code{MPI_Recv} and
@c @code{MPI_Bcast}.
